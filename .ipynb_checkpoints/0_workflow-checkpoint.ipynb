{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33cdb7cc-0186-4bbc-bc10-22b433eca869",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# **Process Ngram Files**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4d1bf3-85d8-477a-a481-ba2edb7fdbe3",
   "metadata": {},
   "source": [
    "## Process multigram files\n",
    "Download multigrams (_n_ = 2–5). You can specify a **vocabulary file** for filtering. Vocabulary filtering dicards ngrams containing tokens absent from the vocabulary file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdef6733-1a42-4884-a438-5a6cc9c36f58",
   "metadata": {},
   "source": [
    "#### Set the appropriate base directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d325d3c2-9b5a-44e2-aa7d-25b436881047",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/vast/edk202/NLP_corpora/Google_Books/20200217/eng\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650f6833-0dbb-4e04-9499-aca32bf644b9",
   "metadata": {},
   "source": [
    "#### Train a basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3288e9d7-cd56-42c0-aa6c-f69a8aeae752",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train_word2vec.py \\\n",
    "    --corpus_file \"{base_dir}/5gram_files/year_files/text/2019.txt\" \\\n",
    "    --model_file \"{base_dir}/5gram_files/word2vec_model.model\" \\\n",
    "    --vector_file \"{base_dir}/5gram_files/word_vectors.txt\" \\\n",
    "    --vector_size 300 \\\n",
    "    --window 5 \\\n",
    "    --sg 1 \\\n",
    "    --negative 10 \\\n",
    "    --min_count 5 \\\n",
    "    --sample 1e-5 \\\n",
    "    --workers 48 \\\n",
    "    --epochs 10 \\\n",
    "    --alpha 0.025 \\\n",
    "    --min_alpha 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b9bfa8-d24d-4972-b9e5-7fc874fde98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load the model\n",
    "model = Word2Vec.load(f'{base_dir}/5gram_files/word2vec_model.model')\n",
    "\n",
    "similar_words = model.wv.most_similar('water_ADJ', topn=10)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47cff99-a6e6-4a36-9ad4-aec0658de8a6",
   "metadata": {},
   "source": [
    "## Generate Vocabulary File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecddde54-96c0-4461-a0f7-6ef2bbecd065",
   "metadata": {},
   "source": [
    "Make a list of the _n_ most common unigrams (1grams). This file is used to filter multi-token ngrams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e579914e-af52-4346-8637-ea4e6be5362d",
   "metadata": {},
   "source": [
    "### Set base directory\n",
    "The scripts need to know where your project is stored, and will add subdirectories there as they go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dea9f17-331d-40b5-b9e9-7bd8f254da64",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/vast/edk202/NLP_corpora/Google_Books/20200217/eng\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97fea36-f682-4a1c-aedc-d905ba30c001",
   "metadata": {},
   "source": [
    "### Download unigrams\n",
    "\n",
    "Downloads unigrams (`--ngram_size 1`) appended with part-of-speech (POS) tags (e.g., `_VERB`). Although you can specify `--ngram_type untagged`, POS tags are necessary to lemmatize the tokens. If storage space is limited, specify `--compress` here and throughout the worlflow; this tells the code to use LZ4 compression when storing output files. Downstream scripts will see the `.lz4` extensions and handle the files accordingly. Specify as many parallel processes as you have available or wish to use by setting `--workers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85c19814-cbee-4ce4-b2ca-0d4abf76d3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mStart Time:                2025-01-01 18:16:37.088160\n",
      "\u001b[0m\n",
      "\u001b[4mDownload Info\u001b[0m\n",
      "Ngram repository:          https://storage.googleapis.com/books/ngrams/books/20200217/eng/eng-1-ngrams_exports.html\n",
      "Output directory:          /vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/1download\n",
      "File index range:          0 to 23\n",
      "File URLs available:       24\n",
      "File URLs to use:          24\n",
      "First file to get:         https://storage.googleapis.com/books/ngrams/books/20200217/eng/1-00000-of-00024.gz\n",
      "Last file to get:          https://storage.googleapis.com/books/ngrams/books/20200217/eng/1-00023-of-00024.gz\n",
      "Ngram size:                1\n",
      "Ngram type:                tagged\n",
      "Number of workers:         48\n",
      "Compress saved files:      True\n",
      "Overwrite existing files:  True\n",
      "\n",
      "Downloading     |\u001b[32m██████████████████████████████████████████████████\u001b[0m| 100.0% 24          /24         \u001b[0m\n",
      "\u001b[31m\n",
      "End Time:                  2025-01-01 18:17:16.648932\u001b[0m\n",
      "\u001b[31mTotal runtime:             0:00:39.560772\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python 1_download.py \\\n",
    "    --ngram_size 1 \\\n",
    "    --ngram_type tagged \\\n",
    "    --proj_dir {base_dir} \\\n",
    "    --workers 48 \\\n",
    "    --overwrite \\\n",
    "    --compress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799d4fd8-2fe9-4fe3-8a37-20f0e1b81eef",
   "metadata": {},
   "source": [
    "### Convert unigrams to JSONL files\n",
    "\n",
    "Convert the original unigram files' text data to a more flexible JSON Lines (JSONL) format. Although this increases storage demands, it makes downstream processing more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84af60d9-2627-4a9c-80e4-dde2e1703d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mStart Time:                2025-01-01 18:18:13.673572\n",
      "\u001b[0m\n",
      "\u001b[4mLowercasing Info\u001b[0m\n",
      "Input directory:           /vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/1download\n",
      "Output directory:          /vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/2convert\n",
      "File index range:          0 to 23\n",
      "Files available:           24\n",
      "Files to use:              24\n",
      "First file to get:         /vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/1download/1-00000-of-00024.txt.lz4\n",
      "Last file to get:          /vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/1download/1-00023-of-00024.txt.lz4\n",
      "Ngram size:                1\n",
      "Ngram type:                tagged\n",
      "Number of workers:         48\n",
      "Compress output files:     True\n",
      "Overwrite existing files:  True\n",
      "Delete input directory:    False\n",
      "\n",
      "\u001b[4mConversion Progress\u001b[0m\n",
      "Converting      |\u001b[32m██████████████████████████████████████████████████\u001b[0m| 100.0% 24          /24         \u001b[0m\n",
      "\u001b[31m\n",
      "End Time:                  2025-01-01 18:20:32.320135\u001b[0m\n",
      "\u001b[31mTotal runtime:             0:02:18.646563\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python 2_convert.py \\\n",
    "    --ngram_size 1 \\\n",
    "    --ngram_type tagged \\\n",
    "    --proj_dir {base_dir} \\\n",
    "    --workers 48 \\\n",
    "    --overwrite \\\n",
    "    --compress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ad8638-1ed1-4ce6-b864-70e17635a3a8",
   "metadata": {},
   "source": [
    "### Make unigrams all lowercase\n",
    "Most use cases will benefit from unigrams that are all lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cc700cf-dba7-40d6-b5f0-e98723af2e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mStart Time:                2025-01-01 18:20:46.699502\n",
      "\u001b[0m\n",
      "\u001b[4mLowercasing Info\u001b[0m\n",
      "Input directory:           /vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/2convert\n",
      "Output directory:          /vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/3lowercase\n",
      "File index range:          0 to 23\n",
      "Files available:           24\n",
      "Files to use:              24\n",
      "First file to get:         /vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/2convert/1-00000-of-00024.jsonl.lz4\n",
      "Last file to get:          /vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/2convert/1-00023-of-00024.jsonl.lz4\n",
      "Ngram size:                1\n",
      "Number of workers:         48\n",
      "Compress output files:     True\n",
      "Overwrite existing files:  True\n",
      "Delete input directory:    False\n",
      "\n",
      "\u001b[4mLowercasing Progress\u001b[0m\n",
      "Lowercasing     |\u001b[32m██████████████████████████████████████████████████\u001b[0m| 100.0% 24          /24         \u001b[0m\n",
      "\u001b[31m\n",
      "End Time:                  2025-01-01 18:21:51.655646\u001b[0m\n",
      "\u001b[31mTotal runtime:             0:01:04.956144\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python 3_lowercase.py \\\n",
    "    --ngram_size 1 \\\n",
    "    --proj_dir {base_dir} \\\n",
    "    --workers 48 \\\n",
    "    --overwrite \\\n",
    "    --compress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8426e48d-76d8-42ff-9e95-7d9c44bdc331",
   "metadata": {},
   "source": [
    "### Lemmatize the uningrams\n",
    "Likewise, most use cases will benefit from unigrams that are lemmatized—that is, reduced to their base form. This requires POS-tagged unigrams. Example: `aggregating_VERB` will be converted to `aggregate` in the output. The POS tag will then be discarded as it is no longer useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82eb3d13-9d51-4ea2-8708-9c3eadb3feae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mStart Time:                2025-01-01 18:22:15.453722\n",
      "\u001b[0m\n",
      "\u001b[4mLemmatizing Info\u001b[0m\n",
      "Input directory:           /vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/3lowercase\n",
      "Output directory:          /vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/4lemmatize\n",
      "File index range:          0 to 23\n",
      "Files available:           24\n",
      "Files to use:              24\n",
      "First file to get:         /vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/3lowercase/1-00000-of-00024.jsonl.lz4\n",
      "Last file to get:          /vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/3lowercase/1-00023-of-00024.jsonl.lz4\n",
      "Ngram size:                1\n",
      "Number of workers:         48\n",
      "Compress output files:     True\n",
      "Overwrite existing files:  True\n",
      "Delete input directory:    False\n",
      "\n",
      "\u001b[4mLemmatization Progress\u001b[0m\n",
      "Lemmatizing     |\u001b[32m██████████████████████████████████████████████████\u001b[0m| 100.0% 24          /24         \u001b[0m\n",
      "\u001b[31m\n",
      "End Time:                  2025-01-01 18:23:36.749628\u001b[0m\n",
      "\u001b[31mTotal runtime:             0:01:21.295906\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python 4_lemmatize.py \\\n",
    "    --ngram_size 1 \\\n",
    "    --proj_dir {base_dir} \\\n",
    "    --workers 48 \\\n",
    "    --overwrite \\\n",
    "    --compress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd52fad3-d372-4613-9c36-b153ca004976",
   "metadata": {},
   "source": [
    "### Filter the unigrams\n",
    "Removes tokens that contain numerals (`--numerals`), nonalphabetic characters (`--nonalpha`), stopwords (`--stopwords`), or short words (`--min_token_length 3`). Since we're processing unigrams, `--min_tokens 1` means than any unigram in the data will be completely discarded. Any empty output files will be deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68104701-700e-4b0d-8b3e-b69127a75ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mStart Time:                   2025-01-01 18:24:38.960769\n",
      "\u001b[0m\n",
      "\u001b[4mFiltering Info\u001b[0m\n",
      "Input directory:              /vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/4lemmatize\n",
      "Output directory:             /vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/5filter\n",
      "File index range:             0 to 23\n",
      "Files available:              24\n",
      "Files to use:                 24\n",
      "First file to get:            /vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/4lemmatize/1-00000-of-00024.jsonl.lz4\n",
      "Last file to get:             /vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/4lemmatize/1-00023-of-00024.jsonl.lz4\n",
      "Ngram size:                   1\n",
      "Number of workers:            48\n",
      "Compress output files:        True\n",
      "Overwrite existing files:     True\n",
      "Delete input directory:       False\n",
      "\n",
      "\u001b[4mFiltering Options\u001b[0m\n",
      "Drop stopwords:               True\n",
      "Drop tokens under:            3 chars\n",
      "Drop tokens with numerals:    True\n",
      "Drop non-alphabetic:          True\n",
      "Drop ngrams under:            1 token(s)\n",
      "\n",
      "\u001b[4mFiltering Progress\u001b[0m\n",
      "Filtering       |\u001b[32m██████████████████████████████████████████████████\u001b[0m| 100.0% 24          /24         \u001b[0m\n",
      "\n",
      "\u001b[4mFiltering Results (Dropped)\u001b[0m\n",
      "Stopword tokens:              5359 \n",
      "Short-word tokens:            22810 \n",
      "Tokens with numerals:         13740955 \n",
      "Tokens with non-alpha chars:  2886365\n",
      "Out-of-vocab tokens:          0\n",
      "Entire ngrams:                16655489 \n",
      "\u001b[31m\n",
      "End Time:                  2025-01-01 18:25:38.147650\u001b[0m\n",
      "\u001b[31mTotal runtime:             0:00:59.186881\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python 5_filter.py \\\n",
    "    --ngram_size 1 \\\n",
    "    --proj_dir {base_dir} \\\n",
    "    --workers 48 \\\n",
    "    --numerals \\\n",
    "    --nonalpha \\\n",
    "    --stopwords \\\n",
    "    --min_token_length 3 \\\n",
    "    --min_tokens 1 \\\n",
    "    --overwrite \\\n",
    "    --compress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffd5722-7816-4c68-89a4-6f824ccb3e1a",
   "metadata": {},
   "source": [
    "### Sort and combine the unigram files\n",
    "\n",
    "Create a single, fully-sorted unigram file out of the filtered files. To create a vocabulary file for filtering multigrams, we need to sort the input files `--descending` by `freq_tot`, giving us a rank-ordered list of unigrams by frequency. \n",
    "\n",
    "This is an expensive process in terms of computations, memory, and storage and is designed to be as economical and efficient as possible. We start by sorting each individual filtered file using Python's standard sorting algorithm (Timsort). Then, we incrementally merge the sorted files in parallel until we get down to 2 files using a heap-merge algorithm. Finally, we heap-merge the final 2 files (necessarily using one process) to arrive at a single combined and sorted unigram file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52e80618-397f-4051-b064-a8b689e77c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mStart Time:                2025-01-01 19:10:57.817487\n",
      "\u001b[0m\n",
      "\u001b[4mSort Info\u001b[0m\n",
      "Input directory:           /vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/5filter\n",
      "Sorted directory:          /vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/temp\n",
      "Temp directory:            /vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/tmp\n",
      "Merged file:               /vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/6corpus/1gram-merged.jsonl.lz4\n",
      "Files available:           18\n",
      "Files to use:              18\n",
      "First file to get:         /vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/5filter/1-00006-of-00024.jsonl.lz4\n",
      "Last file to get:          /vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/5filter/1-00023-of-00024.jsonl.lz4\n",
      "Ngram size:                1\n",
      "Number of workers:         10\n",
      "Compress output files:     True\n",
      "Overwrite existing files:  True\n",
      "Sort key:                  ngram\n",
      "Sort order:                ascending\n",
      "Heap-merge start iter:     1\n",
      "Heap-merge end iter:       None\n",
      "\n",
      "Sorting         |\u001b[32m██████████████████████████████████████████████████\u001b[0m| 100.0% 18          /18         \u001b[0m\n",
      "\n",
      "Iteration 1: merging 18 files into 6 chunks using 6 workers.\n",
      "  6 chunk(s) with 3 file(s)\n",
      "\n",
      "Iteration 2: merging 6 files into 3 chunks using 3 workers.\n",
      "  3 chunk(s) with 2 file(s)\n",
      "\n",
      "Iteration 3: merging 3 files into 2 chunks using 2 workers.\n",
      "  1 chunk(s) with 1 file(s)\n",
      "  1 chunk(s) with 2 file(s)\n",
      "\n",
      "Iteration 4: final merge of 2 files.\n",
      "Merging         |\u001b[32m██████████████████████████████████████████████████\u001b[0m| 100.0% 25127726    /25127726   \u001b[0m\n",
      "\n",
      "Merging complete. Final merged file:\n",
      "/vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/6corpus/1gram-merged.jsonl.lz4\n",
      "\u001b[31m\n",
      "End Time:                  2025-01-01 19:30:13.555189\u001b[0m\n",
      "\u001b[31mTotal runtime:             0:19:15.737702\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python 6_sort2.py \\\n",
    "    --ngram_size 1 \\\n",
    "    --proj_dir {base_dir} \\\n",
    "    --workers 10 \\\n",
    "    --sort_key ngram \\\n",
    "    --compress \\\n",
    "    --overwrite \\\n",
    "    --sort_order ascending"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc33c06-ca7f-4d99-9949-7520ef0e0c98",
   "metadata": {},
   "source": [
    "### Verify sort [OPTIONAL]\n",
    "If we want, we can verify that the output file is correctly sorted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cfd12d0-5dfc-482e-a145-0306b4e01ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines: 25127726line [05:01, 83447.21line/s] \n",
      "\n",
      "The file is sorted.\n",
      "\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "!python verify_sort.py \\\n",
    "    --input_file \"{base_dir}/1gram_files/6corpus/1gram-merged.jsonl.lz4\" \\\n",
    "    --field ngram \\\n",
    "    --sort_order ascending"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353a8ec7-38b8-4d5d-8356-1af9f8e26bf9",
   "metadata": {},
   "source": [
    "### Consolidate duplicate unigrams\n",
    "Lowercasing and lemmatizing produces duplicate unigrams. Now that the file is sorted, we can scan through it and consolidate consecutive idential duplicates. This involves summing their overall and yearly frequencies and document counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8609476-7636-43c6-8945-1eb6b104f999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mStart Time:                2025-01-01 19:42:40.646053\n",
      "\u001b[0m\n",
      "\u001b[4mConsolidation Info\u001b[0m\n",
      "Merged file:               /vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/6corpus/1gram-merged.jsonl.lz4\n",
      "Consolidated directory:    /vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/6corpus/1gram-consolidated.jsonl.lz4\n",
      "Ngram size:                1\n",
      "Compress output files:     True\n",
      "Overwrite existing files:  True\n",
      "\n",
      "Consolidating   |\u001b[32m██████████████████████████████████████████████████\u001b[0m| 100.0% 25127726    /25127726   \u001b[0m\n",
      "\n",
      "Lines before consolidation:  25127726\n",
      "Lines after consolidation:   13499384\n",
      "\u001b[31m\n",
      "End Time:                  2025-01-01 19:54:29.839860\u001b[0m\n",
      "\u001b[31mTotal runtime:             0:11:49.193807\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python 7_consolidate.py \\\n",
    "    --ngram_size 1 \\\n",
    "    --proj_dir {base_dir} \\\n",
    "    --overwrite \\\n",
    "    --compress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da8c4c3-6e45-40c7-af67-7f9eb80b3d21",
   "metadata": {},
   "source": [
    "### View line [OPTIONAL]\n",
    "If we want, we can inspect a line in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8cd93ff1-828e-4e3f-afa7-453c2aea62be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 1000000: {'ngram': 'bandageing', 'freq_tot': 71, 'doc_tot': 51, 'freq': {'1804': 1, '1817': 1, '1845': 2, '1870': 1, '1872': 1, '1873': 1, '1874': 1, '1878': 1, '1885': 5, '1891': 1, '1907': 1, '1911': 2, '1914': 2, '1916': 1, '1917': 24, '1921': 1, '1964': 1, '1966': 1, '1973': 3, '1974': 1, '1988': 1, '1991': 1, '2000': 4, '2004': 2, '2005': 3, '2007': 2, '2012': 2, '2014': 4}, 'doc': {'1804': 1, '1817': 1, '1845': 2, '1870': 1, '1872': 1, '1873': 1, '1874': 1, '1878': 1, '1885': 5, '1891': 1, '1907': 1, '1911': 2, '1914': 2, '1916': 1, '1917': 12, '1921': 1, '1964': 1, '1966': 1, '1973': 2, '1974': 1, '1988': 1, '1991': 1, '2000': 2, '2004': 1, '2005': 2, '2007': 1, '2012': 2, '2014': 2}}\n",
      "Line 1000001: {'ngram': 'bandageless', 'freq_tot': 79, 'doc_tot': 66, 'freq': {'1907': 2, '1909': 10, '1910': 1, '1921': 20, '1926': 1, '1927': 3, '1934': 2, '1935': 1, '1945': 2, '1958': 4, '1969': 5, '1982': 1, '1985': 1, '1987': 1, '1998': 1, '2001': 1, '2003': 1, '2004': 1, '2005': 2, '2007': 1, '2008': 2, '2009': 3, '2010': 1, '2012': 2, '2013': 4, '2014': 1, '2015': 2, '2018': 2, '2019': 1}, 'doc': {'1907': 2, '1909': 10, '1910': 1, '1921': 8, '1926': 1, '1927': 3, '1934': 2, '1935': 1, '1945': 2, '1958': 4, '1969': 5, '1982': 1, '1985': 1, '1987': 1, '1998': 1, '2001': 1, '2003': 1, '2004': 1, '2005': 2, '2007': 1, '2008': 2, '2009': 3, '2010': 1, '2012': 2, '2013': 4, '2014': 1, '2015': 1, '2018': 2, '2019': 1}}\n"
     ]
    }
   ],
   "source": [
    "!python print_jsonl_lines.py \\\n",
    "    --file_path \"{base_dir}/1gram_files/6corpus/1gram-consolidated.jsonl.lz4\" \\\n",
    "    --start 1000000 \\\n",
    "    --end 1000001 \\\n",
    "    --parse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc294ed6-399f-4fbd-91e4-8e55e86950d0",
   "metadata": {},
   "source": [
    "### Index unigrams and create vocabulary file\n",
    "Most use cases will require an indexed list of \"valid\" (i.e., reasonably common) vocabulary words. This indexing script served dual functions of (1) mapping each unigram to an index number (saved in `/6corpus/1gram-consolidated-indexed.jsonl`) and (2) culling this file into a vocabulary list consisting of the _n_ most frequent unigrams (saved in `6corpus/1gram-consolidated-vocab_list_match.txt`). Unlike files upstream in the workflow, the vocabulary files are not very large and are therefore not compressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e858de53-fe04-4a7d-bab5-9eed5cdf0011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mStart Time:                2025-01-01 22:32:01.094067\n",
      "\u001b[0m\n",
      "\u001b[4mIndexing Info\u001b[0m\n",
      "Project directory:         /vast/edk202/NLP_corpora/Google_Books/20200217/eng\n",
      "Output directory:          /vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/6corpus\n",
      "Input file:                /vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/6corpus/1gram-consolidated.jsonl.lz4\n",
      "Ngram size:                1\n",
      "Overwrite existing files:  True\n",
      "Workers:                   48\n",
      "Vocab size (top N):        80000\n",
      "\n",
      "\n",
      "\u001b[4mIndexing Info\u001b[0m\n",
      "Chunking        |\u001b[32m██████████████████████████████████████████████████\u001b[0m| 100.0% 13499384    /13499384   \u001b[0m\n",
      "Sorting         |\u001b[32m██████████████████████████████████████████████████\u001b[0m| 100.0% 135         /135        \u001b[0m\n",
      "Merging         |\u001b[32m██████████████████████████████████████████████████\u001b[0m| 100.0% 13499384    /13499384   \u001b[0m\n",
      "Indexing        |\u001b[32m██████████████████████████████████████████████████\u001b[0m| 100.0% 13499384    /13499384   \u001b[0m\n",
      "\n",
      "Indexed 13499384 lines.\n",
      "Final indexed file: 1gram-consolidated-indexed.jsonl\n",
      "Created vocab_list_match and vocab_list_lookup files for top 80000 ngrams.\n",
      "\u001b[31m\n",
      "End Time:                  2025-01-01 22:42:56.494395\u001b[0m\n",
      "\u001b[31mTotal runtime:             0:10:55.400328\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python 7_index.py \\\n",
    "    --ngram_size 1 \\\n",
    "    --proj_dir {base_dir} \\\n",
    "    --input_file {base_dir}/1gram_files/6corpus/1gram-consolidated.jsonl.lz4 \\\n",
    "    --overwrite \\\n",
    "    --vocab_file 80000 \\\n",
    "    --workers 48"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5015c4-837e-4373-a3ef-0e31ae7b9b77",
   "metadata": {},
   "source": [
    "## Process Multigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feb02beb-cf0e-4beb-ba15-c18a5db621b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/vast/edk202/NLP_corpora/Google_Books/20200217/eng\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d990d95e-7676-4f91-bbbe-ee69164fc918",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python 1_download.py \\\n",
    "    --ngram_size 5 \\\n",
    "    --ngram_type tagged \\\n",
    "    --proj_dir {base_dir} \\\n",
    "    --overwrite \\\n",
    "    --compress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade8be77-ed83-412a-ae1c-c1a71b9d4de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python 2_convert.py \\\n",
    "    --ngram_size 5 \\\n",
    "    --ngram_type tagged \\\n",
    "    --proj_dir {base_dir} \\\n",
    "    --compress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1f6316-f622-4a91-a8ed-fcf23acf222f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python 3_lowercase.py \\\n",
    "    --ngram_size 5 \\\n",
    "    --proj_dir {base_dir} \\\n",
    "    --compress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf64680-b402-4b4d-b751-90046b87454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python 4_lemmatize.py \\\n",
    "    --ngram_size 5 \\\n",
    "    --proj_dir {base_dir} \\\n",
    "    --compress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb82465-1aaf-4e17-9a95-0d0c0c58d585",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python 5_filter.py \\\n",
    "    --ngram_size 5 \\\n",
    "    --proj_dir {base_dir} \\\n",
    "    --numerals \\\n",
    "    --nonalpha \\\n",
    "    --stopwords \\\n",
    "    --min_token_length 3 \\\n",
    "    --min_tokens 2 \\\n",
    "    --compress \\\n",
    "    --vocab_file {base_dir}/1gram_files/6corpus/1-00000-to-00017-vocab_list_match.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b1069b-c9ee-454a-82f9-893662094fbf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mStart Time:                2025-01-01 15:54:18.573790\n",
      "\u001b[0m\n",
      "\u001b[4mSort Info\u001b[0m\n",
      "Input directory:           /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5filter\n",
      "Sorted directory:          /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/temp\n",
      "Temp directory:            /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/tmp\n",
      "Merged file:               /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/6corpus/5gram-merged.jsonl.lz4\n",
      "Files available:           6520\n",
      "Files to use:              6520\n",
      "First file to get:         /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5filter/5-00069-of-19423.jsonl.lz4\n",
      "Last file to get:          /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5filter/5-19422-of-19423.jsonl.lz4\n",
      "Ngram size:                5\n",
      "Number of workers:         10\n",
      "Compress output files:     True\n",
      "Overwrite existing files:  False\n",
      "Sort key:                  ngram\n",
      "Sort order:                ascending\n",
      "Heap-merge start iter:     1\n",
      "Heap-merge end iter:       2\n",
      "\n",
      "Sorting         |\u001b[32m████████████████████▎                             \u001b[0m|  40.6% 2650        /6520       \u001b[0m"
     ]
    }
   ],
   "source": [
    "base_dir = \"/vast/edk202/NLP_corpora/Google_Books/20200217/eng\"\n",
    "\n",
    "!python 6_sort2.py \\\n",
    "    --ngram_size 5 \\\n",
    "    --proj_dir {base_dir} \\\n",
    "    --workers 10 \\\n",
    "    --sort_key ngram \\\n",
    "    --compress \\\n",
    "    --sort_order ascending \\\n",
    "    --end_iteration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96f0706e-fc96-4bb8-a821-cb3d91299441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mStart Time:                2024-12-31 17:52:08.091974\n",
      "\u001b[0m\n",
      "\u001b[4mConsolidation Info\u001b[0m\n",
      "Merged file:               /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/6corpus/5gram-merged.jsonl.lz4\n",
      "Consolidated directory:    /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/6corpus/5gram-consolidated.jsonl.lz4\n",
      "Ngram size:                5\n",
      "Compress output files:     True\n",
      "Overwrite existing files:  True\n",
      "\n",
      "Consolidating   |\u001b[32m██████████████████████████████████████████████████\u001b[0m| 100.0% 276470316   /276470316  \u001b[0m\n",
      "\n",
      "Lines before consolidation:  276470316\n",
      "Lines after consolidation:   75107076\n",
      "\u001b[31m\n",
      "End Time:                  2024-12-31 19:10:05.741234\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python 7_consolidate.py \\\n",
    "    --ngram_size 5 \\\n",
    "    --proj_dir {base_dir} \\\n",
    "    --overwrite \\\n",
    "    --compress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3271194c-308f-4446-8573-ea13a0f278da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import lz4.frame\n",
    "\n",
    "input_path = \"/vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/6corpus/5gram-consolidated.jsonl.lz4\"\n",
    "output_path = \"/vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/6corpus/5gram-consolidated.jsonl\"\n",
    "\n",
    "def decompress_lz4_file(input_path, output_path):\n",
    "    with lz4.frame.open(input_path, \"rb\") as compressed_file:\n",
    "        with open(output_path, \"wb\") as decompressed_file:\n",
    "            decompressed_file.write(compressed_file.read())\n",
    "\n",
    "decompress_lz4_file(input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7136660-a7a4-4bbf-b585-519681c1aa08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading the file '/vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/6corpus/1gram-consolidated.jsonl': [Errno 2] No such file or directory: '/vast/edk202/NLP_corpora/Google_Books/20200217/eng/1gram_files/6corpus/1gram-consolidated.jsonl'\n"
     ]
    }
   ],
   "source": [
    "!python print_jsonl_lines.py \\\n",
    "    --file_path \"{base_dir}/1gram_files/6corpus/1gram-consolidated.jsonl\" \\\n",
    "    --start 50000 \\\n",
    "    --end 50100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90c543c8-0db9-4901-916b-890d95c2ebf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines: 75107076line [08:47, 142445.80line/s]\n",
      "\n",
      "The file is sorted.\n",
      "\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "!python verify_sort.py \\\n",
    "    --input_file \"{base_dir}/5gram_files/6corpus/5gram-consolidated.jsonl\" \\\n",
    "    --field ngram \\\n",
    "    --sort_order ascending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58bfd86-dd46-417a-9bd3-9d52bd8a4fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python simulate_merge.py \\\n",
    "    --file_dir \"{base_dir}/5gram_files/temp\" \\\n",
    "    --workers 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aba9af-704f-4940-a4bf-1f45a1d4be1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/vast/edk202/NLP_corpora/Google_Books/20200217/eng\"\n",
    "\n",
    "!python simulate_merge2.py \\\n",
    "    --ngram_size 5 \\\n",
    "    --file_dir \"{base_dir}/5gram_files/temp\" \\\n",
    "    --tmp_dir \"{base_dir}/5gram_files/tmp\" \\\n",
    "    --compress \\\n",
    "    --sort_key ngram \\\n",
    "    --sort_order ascending \\\n",
    "    --workers 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0445d4a-d9f6-4864-b8ae-d5e762913fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/vast/edk202/NLP_corpora/Googl : decoded 18959398668 bytes                     \n"
     ]
    }
   ],
   "source": [
    "!lz4 -t {base_dir}/1gram_files/6corpus/1gram-consolidated.jsonl.lz4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fb72a9-746d-49e4-b54b-36318fd942a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hist_w2v_env",
   "language": "python",
   "name": "hist_w2v_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
