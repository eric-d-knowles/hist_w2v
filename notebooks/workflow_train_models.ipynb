{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "943ad2f1-f36b-4d0a-945c-d8998ca9348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_tools.train_models import train_models\n",
    "from training_tools.evaluate_models import evaluate_models\n",
    "from training_tools.plotting import load_results, plot_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2764de0-eb1b-456f-a0a5-4ebd8eca09ec",
   "metadata": {},
   "source": [
    "# **Train `Word2vec` Models**\n",
    "## **Goal**: Train and evaluate word embeddings using the `Word2vec` algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074aae39-7895-4548-8c1e-06ddff3e562a",
   "metadata": {},
   "source": [
    "If you have successfully run the two previous workflows (`workflow_unigrams.ipynb` and `workflow_multigrams.ipynb`), then you are in possession of pre-processed yearly Google Ngram data. This workflow uses the `train_models.py` module to train year-specific models using [Word2Vec](https://en.wikipedia.org/wiki/Word2vec)—a popular technique for deriving [vector representations](https://en.wikipedia.org/wiki/Word_embedding) of the words in a corpus. You can then use the `evaluate_models.py` module to see how well your models have been trained.\n",
    "\n",
    "Training word-embedding models requires decisions about various modeling parameters (i.e., hyperparameters). The most important are:\n",
    "\n",
    "1. `vector_size`: the dimensionality of the vector space that words are embedded in. Think of this as the number of other words in relation to which the model understands any single word. More dimensions are not necessarily better. While too few dimensions can prevent the model from learning words' full meanings, too many dimensions risks overfitting or allowing low-frequency, specialized uses to obscure words' most important meanings.\n",
    "\n",
    "2. `window`: the width of `word2vec`'s sliding context window. Word2vec reads in \"sentences\" (in this case, multigrams) and tries to learn probabilistic relationships between a target word and the words that surround it in the corpus. To the extent that two words have the same probabilistic relations to other nearby words, their vector representations will be similar. The `window` parameter determines what \"nearby\" means: With a window of 2, a word's meaning is defined strictly by the words immediately adjacent to it; with a window of 5, a word's meaning is determined by words up to four words away. Roughly speaking, models trained using a narrow context window will privilege _syntactic_ relationships, whereas models trained using a wider context window will learn _semantic_ relationships.\n",
    "\n",
    "3. `approach`: the training architecture to use. You can specify either `CBOW` (Continuous Bag of Words) or `skip-gram`. In CBOW, context words are used to predict target words; in `skip-gram`, target words are used to predict context words. `skip-gram` yields better results with ngrams.\n",
    "\n",
    "4. `min_count`: the minimum number of times a word must appear in the corpus to be used for training. You may wish to ignore extremely infrequent words—especially in large corpora. This parameter let's you do this.\n",
    "\n",
    "5. `weight_by`: the strategy for weighting ngrams by their frequency in the corpus. `none` means that no weighting is used and each unique ngram is fed to the model only once. `freq` gives ngrams a \"bonus\" if they appear multiple times; however the bonus diminshes as the frequency increases. For example, an ngram appearing 100 times in the corpus will be fed to `word2vec` twice, an ngram appearing 1,000 times is fed to the model 3 times, an ngram appearing 10,000 is fed to the model 4 times, and so on. The `doc_freq` option does the same thing, but using the number of unique _documents_ ngrams appear in. The goal of weighting is to allow somewhat frequent ngrams to influence the model more than rare ones—while not letting extremely frequent ngrams skew the model.\n",
    "\n",
    "6. `epochs`: the number of training passes over the corpus. Too many can lead to overfitting, while too few can lead to imprecise embeddings. Experiment to see which values lead to the best model performance.\n",
    "\n",
    "The other options are straightforward. `proj_dir` is the base directory for your project, `years=([start_year], [end_year])` specifies which yearly models to train, and `workers` is the number of processes to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8bf77a-cc46-412a-a782-c37b2fd70686",
   "metadata": {},
   "source": [
    "### Train Models\n",
    "The `train_models.py` module can be used to iterate through multiple parameter combinations—allowing you to conduct a [grid search](https://en.wikipedia.org/wiki/Hyperparameter_optimization). Thus, if you want to train models for 2018 and 2019 using vector sizes of 100, 200, and 300 and all three weighting strategies, you would specify `year=(2018, 2019)`, `weight_by=('none', 'freq', 'doc_freq')` and `vector_size=(100, 200, 300)`. The module would then train models using all combinations of these parameters, for a total of 18 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb620c8-7c6a-4022-9025-a354809c4895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mStart Time:         2025-01-21 07:32:34.218644\n",
      "\u001b[0m\n",
      "\u001b[4mTraining Info\u001b[0m\n",
      "Data directory:     /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/6corpus/yearly_files/data\n",
      "Model directory:    /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/6corpus/yearly_files/models\n",
      "Log directory:      /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/6corpus/yearly_files/logs/training\n",
      "Ngram size:         5\n",
      "Number of workers:  48\n",
      "\n",
      "Grid paramters:\n",
      "  Weighting:           ('none',)\n",
      "  Vector size:         (400, 500)\n",
      "  Context window:      (5,)\n",
      "  Minimum word count:  (1,)\n",
      "  Approach:            ('skip-gram',)\n",
      "  Training epochs:     (11, 12, 13, 14, 15)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63faf157e4ba47ec872c455fbfb8fa54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Models:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_models(\n",
    "    ngram_size=5,\n",
    "    proj_dir='/vast/edk202/NLP_corpora/Google_Books/20200217/eng/',\n",
    "    years=(2019, 2019),\n",
    "    weight_by='none',\n",
    "    vector_size=(400, 500),\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    approach='skip-gram',\n",
    "    epochs=(11, 12, 13, 14, 15)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2272f65d-ec12-4fc7-9ad4-de907946dc5e",
   "metadata": {},
   "source": [
    "### Evaluate Training Quality\n",
    "The next step is to examine the quality of the trained models. The `evaluate_models.py` module performs two \"intrinsic\" tests on the models in your project's `models` directory: a similarity test and an analogy test. In a similarity test, models predict human-rated similarities between word pairs. In an analogy test, models attempt to answer questions of the form \"_a_ is to _b_ as _c_ is to what?\"—for example, \"_king_ is to _queen_ as _man_ is to what?\" (where the correct answer is _woman_).\n",
    "\n",
    "By default, the code runs the similarity tests packaged with Gensim, but if you want to use different test items, you can by specifying the `similarity_dataset` and `analogy_dataset` options (making sure that the test files are properly formatted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc580fa0-5184-4a9a-a3df-1e464ad67ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_models(\n",
    "    ngram_size=5,\n",
    "    proj_dir='/vast/edk202/NLP_corpora/Google_Books/20200217/eng/',\n",
    "    eval_dir='/scratch/edk202/hist_w2v/training_results',\n",
    "    save_mode='append',\n",
    "    workers=24\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56703bbb-8f41-44eb-a666-181ebc61b4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = load_results('../training_results/evaluation_results.csv')\n",
    "\n",
    "plot_metrics(\n",
    "    df=results_df,\n",
    "    x_vars=['vector_size', 'epochs'],\n",
    "    plot_type='contour',\n",
    "    metric='similarity_score'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c11d968-b4a8-4e93-9b3e-a63b79150de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = load_results('../training_results/evaluation_results.csv')\n",
    "\n",
    "plot_metrics(\n",
    "    df=results_df,\n",
    "    x_vars=['vector_size', 'epochs'],\n",
    "    plot_type='contour',\n",
    "    metric='analogy_score'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hist_w2v_env",
   "language": "python",
   "name": "hist_w2v_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
