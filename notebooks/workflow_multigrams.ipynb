{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7703a5f-b7f5-4c07-a6c7-b8d726792012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "src_path = Path.cwd().parent / \"src\"\n",
    "sys.path.append(str(src_path))\n",
    "\n",
    "from ngram_tools.download_ngrams import download_ngram_files\n",
    "from ngram_tools.convert_to_jsonl import convert_to_jsonl_files\n",
    "from ngram_tools.lowercase_ngrams import lowercase_ngrams\n",
    "from ngram_tools.lemmatize_ngrams import lemmatize_ngrams\n",
    "from ngram_tools.filter_ngrams import filter_ngrams\n",
    "from ngram_tools.sort_ngrams import sort_ngrams\n",
    "from ngram_tools.helpers.verify_sort import check_file_sorted\n",
    "from ngram_tools.consolidate_ngrams import consolidate_duplicate_ngrams\n",
    "from ngram_tools.helpers.print_jsonl_lines import print_jsonl_lines\n",
    "from ngram_tools.index_and_create_vocab import index_and_create_vocab_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cdb7cc-0186-4bbc-bc10-22b433eca869",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# **Process Multigrams for Training Word-Embedding Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e915c8-5224-4cbe-b5a2-b58978dd48f7",
   "metadata": {},
   "source": [
    "## **Goal**: Download and preprocess mulitgrams for use in training `Word2Vec` models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794a3cfb-8fee-4303-a2f9-93786529e21d",
   "metadata": {},
   "source": [
    "This workflow is resource-intensive and is probably only practical when run on a computing cluster. On my university's High Performance Computing (HPC) cluster, I request the maximum 14 cores (48 logical processors) and 128G of memory and use a 2T fast-I/O NVMe SSD filespace. I still run up against time and time and resource limits and have designed the code to contend with them.\n",
    "\n",
    "The code affords some options to keep things efficient. Throughout the workflow you can specify compress=True, which tells the code to compress its output files. Downstream scripts will see the `.lz4` extensions and handle the files accordingly. If you know your workflow runs correctly and wish to further conserve space, you can specify `delete_input=True` for many of the scripts; this will delete the source files for a given step once that step is complete. The scripts are fairly memory-efficient—with the exception of `sort_ngrams` and `index_and_create_vocab_files`, which sort multiple files in memory at once. When processing multigrams, I've found that allocating more than ~10 workers in these scripts leads to memory exhaustion (with 128G!) and slow processing.\n",
    "\n",
    "**NOTE:** You'll probably want to have run the unigram workflow before processing multigrams. That workflow allows you create a vocabulary file for filtering out uncommon tokens from the multigrams. Although you can run the `filter_ngrams` module without a vocab file, most use cases will benefit from one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97fea36-f682-4a1c-aedc-d905ba30c001",
   "metadata": {},
   "source": [
    "### Download multigrams\n",
    "Here, I'm using `download_ngrams` module to fetch 5grams appended with part-of-speech (POS) tags (e.g., `_VERB`). Although you can specify `ngram_type='untagged'`, POS tags are necessary to lemmatize the tokens. Specify the number of parallel processes you wish to use by setting `workers` (the default is all available processors). I've specified `compress=True` becausae 5gram files are _big_.\n",
    "\n",
    "`[Runtime: 4:19:11.661609]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85c19814-cbee-4ce4-b2ca-0d4abf76d3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mStart Time:                2025-01-04 17:43:25.557908\n",
      "\u001b[0m\n",
      "\u001b[4mDownload Info\u001b[0m\n",
      "Ngram repository:          https://storage.googleapis.com/books/ngrams/books/20200217/eng/eng-5-ngrams_exports.html\n",
      "Output directory:          /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/1download\n",
      "File index range:          0 to 19422\n",
      "File URLs available:       19423\n",
      "File URLs to use:          19423\n",
      "First file to get:         https://storage.googleapis.com/books/ngrams/books/20200217/eng/5-00000-of-19423.gz\n",
      "Last file to get:          https://storage.googleapis.com/books/ngrams/books/20200217/eng/5-19422-of-19423.gz\n",
      "Ngram size:                5\n",
      "Ngram type:                tagged\n",
      "Number of workers:         48\n",
      "Compress saved files:      True\n",
      "Overwrite existing files:  False\n",
      "\n",
      "Downloading     |\u001b[32m██████████████████████████████████████████████████\u001b[0m| 100.0% 19423       /19423      \u001b[0m\n",
      "\u001b[31m\n",
      "End Time:                  2025-01-04 22:02:37.219517\u001b[0m\n",
      "\u001b[31mTotal runtime:             4:19:11.661609\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "download_ngram_files(\n",
    "    ngram_size=5,\n",
    "    ngram_type='tagged',\n",
    "    proj_dir='/vast/edk202/NLP_corpora/Google_Books/20200217/eng',\n",
    "    compress=True,\n",
    "    overwrite=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799d4fd8-2fe9-4fe3-8a37-20f0e1b81eef",
   "metadata": {},
   "source": [
    "### Convert files from TXT to JSONL\n",
    "This module converts the original multigram files' text data to a more flexible JSON Lines (JSONL) format. Although this increases storage demands, it makes downstream processing more efficient.\n",
    "\n",
    "`[Total runtime: 1:30:56.083010]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84af60d9-2627-4a9c-80e4-dde2e1703d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mStart Time:                2025-01-05 13:47:05.912405\n",
      "\u001b[0m\n",
      "\u001b[4mConversion Info\u001b[0m\n",
      "Input directory:           /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/1download\n",
      "Output directory:          /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/2convert\n",
      "File index range:          0 to 19422\n",
      "Files available:           19423\n",
      "Files to use:              19423\n",
      "First file to get:         /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/1download/5-00000-of-19423.txt.lz4\n",
      "Last file to get:          /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/1download/5-19422-of-19423.txt.lz4\n",
      "Ngram size:                5\n",
      "Ngram type:                tagged\n",
      "Number of workers:         48\n",
      "Compress output files:     True\n",
      "Overwrite existing files:  False\n",
      "Delete input directory:    True\n",
      "\n",
      "Converting      |\u001b[32m██████████████████████████████████████████████████\u001b[0m| 100.0% 19423       /19423      \u001b[0m\n",
      "\u001b[31m\n",
      "End Time:                  2025-01-05 15:19:30.905972\u001b[0m\n",
      "\u001b[31mTotal runtime:             1:32:24.993567\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "convert_to_jsonl_files(\n",
    "    ngram_size=5,\n",
    "    ngram_type='tagged',\n",
    "    proj_dir='/vast/edk202/NLP_corpora/Google_Books/20200217/eng',\n",
    "    compress=True,\n",
    "    overwrite=False,\n",
    "    delete_input=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ad8638-1ed1-4ce6-b864-70e17635a3a8",
   "metadata": {},
   "source": [
    "### Make multigrams all lowercase\n",
    "This module lowercases all characters in the multigrams. Most use cases benefit from this.\n",
    "\n",
    "`[Total runtime: 0:47:33.859616]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cc700cf-dba7-40d6-b5f0-e98723af2e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mStart Time:                2025-01-05 15:19:31.052375\n",
      "\u001b[0m\n",
      "\u001b[4mLowercasing Info\u001b[0m\n",
      "Input directory:           /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/2convert\n",
      "Output directory:          /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/3lowercase\n",
      "File index range:          0 to 6520\n",
      "Files available:           6521\n",
      "Files to use:              6521\n",
      "First file to get:         /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/2convert/5-00000-of-19423.jsonl.lz4\n",
      "Last file to get:          /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/2convert/5-19422-of-19423.jsonl.lz4\n",
      "Ngram size:                5\n",
      "Number of workers:         48\n",
      "Compress output files:     True\n",
      "Overwrite existing files:  False\n",
      "Delete input directory:    True\n",
      "\n",
      "Lowercasing     |\u001b[32m██████████████████████████████████████████████████\u001b[0m| 100.0% 6521        /6521       \u001b[0m\n",
      "\u001b[31m\n",
      "End Time:                  2025-01-05 16:07:13.575473\u001b[0m\n",
      "\u001b[31mTotal runtime:             0:47:42.523098\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "lowercase_ngrams(\n",
    "    ngram_size=5,\n",
    "    proj_dir='/vast/edk202/NLP_corpora/Google_Books/20200217/eng',\n",
    "    compress=True,\n",
    "    overwrite=False,\n",
    "    delete_input=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8426e48d-76d8-42ff-9e95-7d9c44bdc331",
   "metadata": {},
   "source": [
    "### Lemmatize the multigrams\n",
    "Likewise, most use cases will benefit from multigrams that are lemmatized—that is, reduced to their base form. This requires POS-tagged multigrams. Example: `people_NOUN` (\"the people of this land\") will be converted to `person` in the output; `people_VERB` (\"to people this land\") will not. The POS tag will then be discarded as it is no longer useful.\n",
    "\n",
    "`[Total runtime: 1:39:30.015704]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82eb3d13-9d51-4ea2-8708-9c3eadb3feae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mStart Time:                2025-01-05 16:40:58.283173\n",
      "\u001b[0m\n",
      "\u001b[4mLemmatizing Info\u001b[0m\n",
      "Input directory:           /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/3lowercase\n",
      "Output directory:          /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/4lemmatize\n",
      "File index range:          0 to 6520\n",
      "Files available:           6521\n",
      "Files to use:              6521\n",
      "First file to get:         /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/3lowercase/5-00000-of-19423.jsonl.lz4\n",
      "Last file to get:          /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/3lowercase/5-19422-of-19423.jsonl.lz4\n",
      "Ngram size:                5\n",
      "Number of workers:         48\n",
      "Compress output files:     True\n",
      "Overwrite existing files:  False\n",
      "Delete input directory:    True\n",
      "\n",
      "Lemmatizing     |\u001b[32m██████████████████████████████████████████████████\u001b[0m| 100.0% 6521        /6521       \u001b[0m\n",
      "\u001b[31m\n",
      "End Time:                  2025-01-05 18:18:46.729100\u001b[0m\n",
      "\u001b[31mTotal runtime:             1:37:48.445927\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "lemmatize_ngrams(\n",
    "    ngram_size=5,\n",
    "    proj_dir='/vast/edk202/NLP_corpora/Google_Books/20200217/eng',\n",
    "    compress=True,\n",
    "    overwrite=False,\n",
    "    delete_input=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd52fad3-d372-4613-9c36-b153ca004976",
   "metadata": {},
   "source": [
    "### Filter the multigrams\n",
    "This module removes tokens that provide little information about words' semantic context—specifically, those that contain numerals (`numerals=True`), nonalphabetic characters (`nonalpha=True`), stopwords (high-frequency, low information tokens like \"the\" and \"into\"; `stops=True`), or short words (those below a certain user-specified character count; here, `min_token_length=3`). You can also specify a **vocabulary file** like the one illustrated in the unigram workflow. A vocabulary file is simply a list of the _N_ most common words in the unigram corpus; the multigram tokens are checked against this list and those that don't appear in it are dropped.\n",
    "\n",
    "The filtering process will inevitably turn some longer ngrams (e.g., 5grams) into shorter ones (e.g., 3grams) after unwanted tokens are dropped. The training of word-embedding models requires _linguistic context_—which in turn requires ngrams containing more than one token. A unigram isn't useful for helping a model learn what \"company\" a word keeps. Thus, the `min_tokens` option allows you to drop ngrams that fall below a specified length during filtering. If filtering results in an ngram with fewer than the minimum tokens, all data for that ngram is dropped entirely. Here, I've set `min_tokens=2`, since two tokens (and higher) provide at least some contextual information.\n",
    "\n",
    "`[Total runtime: 0:49:53.902454]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68104701-700e-4b0d-8b3e-b69127a75ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mStart Time:                   2025-01-05 18:18:46.742270\n",
      "\u001b[0m\n",
      "\u001b[4mFiltering Info\u001b[0m\n",
      "Input directory:              /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/4lemmatize\n",
      "Output directory:             /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5filter\n",
      "File index range:             0 to 6520\n",
      "Files available:              6521\n",
      "Files to use:                 6521\n",
      "First file to get:            /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/4lemmatize/5-00000-of-19423.jsonl.lz4\n",
      "Last file to get:             /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/4lemmatize/5-19422-of-19423.jsonl.lz4\n",
      "Ngram size:                   5\n",
      "Number of workers:            48\n",
      "Compress output files:        True\n",
      "Overwrite existing files:     False\n",
      "Delete input directory:       True\n",
      "\n",
      "\u001b[4mFiltering Options\u001b[0m\n",
      "Drop stopwords:               True\n",
      "Drop tokens under:            3 chars\n",
      "Drop tokens with numerals:    True\n",
      "Drop non-alphabetic:          True\n",
      "Drop ngrams under:            2 token(s)\n",
      "\n",
      "Filtering       |\u001b[32m██████████████████████████████████████████████████\u001b[0m| 100.0% 6521        /6521       \u001b[0m\n",
      "\n",
      "\u001b[4mFiltering Results (Dropped)\u001b[0m\n",
      "Stopword tokens:              5112088334 \n",
      "Short-word tokens:            72396856 \n",
      "Tokens with numerals:         217534304 \n",
      "Tokens with non-alpha chars:  1638813401\n",
      "Out-of-vocab tokens:          0\n",
      "Entire ngrams:                497295825 \n",
      "\u001b[31m\n",
      "End Time:                  2025-01-05 19:08:40.644724\u001b[0m\n",
      "\u001b[31mTotal runtime:             0:49:53.902454\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "filter_ngrams(\n",
    "    ngram_size=5,\n",
    "    proj_dir='/vast/edk202/NLP_corpora/Google_Books/20200217/eng',\n",
    "    numerals=True,\n",
    "    nonalpha=True,\n",
    "    stops=True,\n",
    "    min_token_length=3,\n",
    "    min_tokens=2,\n",
    "    vocab_file='/vast/edk202/NLP_corpora/Googl_Books/20200217/eng/',\n",
    "    compress=True,\n",
    "    overwrite=False,\n",
    "    delete_input=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffd5722-7816-4c68-89a4-6f824ccb3e1a",
   "metadata": {},
   "source": [
    "### Sort and combine the multigram files\n",
    "This modules creates a single, fully-sorted multigram file out of the filtered files. This is crucial for the next step (ngram consolidation; see below).   \n",
    "\n",
    "Sorting a giant file is a resource-hungry process and I've tried to implement an efficient approach that leverages parallelism: We first sort the filtered files in parallel using Python's standard sorting algorithm [Timsort](https://en.wikipedia.org/wiki/Timsort); then, we incrementally [heapsort](https://en.wikipedia.org/wiki/Heapsort) the files in parallel until we get down to 2 files. Finally, we heapsort the final 2 files (necessarily using one processor) to arrive at a single combined and sorted unigram file.\n",
    "\n",
    "Because this step can take a _very_ long time for larger multigrams (e.g., 5grams), we can run it in sessions using the `start_iteration` and `end_iteration` options. Iteration 1 comes after the initial file sort. If you only have time to complete, say, iterations 1–3, you can set `end_iteration=3`. During a later session, you can specify `start_iteration=4` to pick up where you left off.\n",
    "\n",
    "`[Sort + Iteration 1 runtime: 1:44:39.319527]`\n",
    "\n",
    "`[Iteration 2 runtime:        0:47:37.115564]`\n",
    "\n",
    "`[Iteration 3 runtime:        1:01:02.813422]`\n",
    "\n",
    "`[Iteration 4 runtime:        1:14:05.632074]`\n",
    "\n",
    "`[Iteration 5 runtime:        2:21:00.000000]`\n",
    "\n",
    "`[Iteration 6 runtime:        4:26:40.465811]`\n",
    "\n",
    "`[Iteration 7 runtime:        7:00:51.409004]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52e80618-397f-4051-b064-a8b689e77c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mStart Time:                2025-01-07 14:48:30.163232\n",
      "\u001b[0m\n",
      "\u001b[4mSort Info\u001b[0m\n",
      "Input directory:           /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5filter\n",
      "Sorted directory:          /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/temp\n",
      "Temp directory:            /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/tmp\n",
      "Merged file:               /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/6corpus/5gram-merged.jsonl.lz4\n",
      "Files available:           6520\n",
      "First file to get:         /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5filter/5-00069-of-19423.jsonl.lz4\n",
      "Last file to get:          /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5filter/5-19422-of-19423.jsonl.lz4\n",
      "Files to use:              6520\n",
      "Ngram size:                5\n",
      "Number of workers:         10\n",
      "Compress output files:     True\n",
      "Overwrite existing files:  False\n",
      "Sort key:                  ngram\n",
      "Sort order:                ascending\n",
      "Heap-merge start iter:     7\n",
      "Heap-merge end iter:       7\n",
      "Deleted sorted files:      True\n",
      "\n",
      "Sorting         |\u001b[32m██████████████████████████████████████████████████\u001b[0m| 100.0% 6520        /6520       \u001b[0m\n",
      "\n",
      "Iteration 7: final merge of 2 files.\n",
      "Merging         |\u001b[32m██████████████████████████████████████████████████\u001b[0m| 100.0% 1982834606  /1982834606 \u001b[0m\n",
      "\n",
      "Merging complete. Final merged file:\n",
      "/vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/6corpus/5gram-merged.jsonl.lz4\n",
      "\u001b[31m\n",
      "End Time:                  2025-01-07 21:49:21.572236\u001b[0m\n",
      "\u001b[31mTotal runtime:             7:00:51.409004\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "sort_ngrams(\n",
    "    ngram_size=5,\n",
    "    proj_dir='/vast/edk202/NLP_corpora/Google_Books/20200217/eng',\n",
    "    workers=10,\n",
    "    sort_key='ngram',\n",
    "    compress=True,\n",
    "    overwrite=False,\n",
    "    sort_order='ascending',\n",
    "    start_iteration=7,\n",
    "    end_iteration=7,\n",
    "    delete_input=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bb548b-a4c8-47b6-ae51-56c79cbb4331",
   "metadata": {},
   "source": [
    "### Verify sort [OPTIONAL]\n",
    "If we want, we can verify that the output file is correctly sorted. If the script outputs `True`, then the file is sorted. Bear in mind that you need to specify the file path manually here; be sure to use the right file extension based on whether `sort_ngrams` was run with `compress=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfd12d0-5dfc-482e-a145-0306b4e01ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_file_sorted(\n",
    "    input_file='/vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/6corpus/5gram-merged.jsonl.lz4',\n",
    "    field=\"ngram\",\n",
    "    sort_order=\"ascending\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353a8ec7-38b8-4d5d-8356-1af9f8e26bf9",
   "metadata": {},
   "source": [
    "### Consolidate duplicate multigrams\n",
    "This module consolidates the sorted multigram file. Lowercasing and lemmatizing produce duplicate unigrams. Now that the file is sorted, we can scan through it and consolidate consecutive idential duplicates. This involves summing their overall and yearly frequencies and document counts. It also leads to a much smaller file.\n",
    "\n",
    "`[Runtime: 0:12:33.459340]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8609476-7636-43c6-8945-1eb6b104f999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating and Processing Chunks: 1982 chunks [4:25:35,  8.04s/ chunks]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging chunks...\n",
      "Merging completed.\n",
      "\n",
      "End Time: 2025-01-09 05:09:36.088154\n",
      "Total runtime: 4:58:07.036475\n"
     ]
    }
   ],
   "source": [
    "consolidate_duplicate_ngrams(\n",
    "    ngram_size=5,\n",
    "    proj_dir='/vast/edk202/NLP_corpora/Google_Books/20200217/eng',\n",
    "    workers=14,\n",
    "    lines_per_chunk=1000000,\n",
    "    compress=True,\n",
    "    overwrite=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da8c4c3-6e45-40c7-af67-7f9eb80b3d21",
   "metadata": {},
   "source": [
    "### View line [OPTIONAL]\n",
    "If we want, we can inspect a line in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cd93ff1-828e-4e3f-afa7-453c2aea62be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 1100050: {'ngram': 'lumbar puncture yield turbid fluid', 'freq_tot': 45, 'doc_tot': 44, 'freq': {'1906': 1, '1908': 7, '1909': 2, '1912': 4, '1914': 1, '1915': 5, '1916': 9, '1928': 3, '1931': 2, '1932': 1, '1933': 1, '1938': 2, '1941': 2, '1944': 1, '1946': 1, '1947': 1, '1964': 1, '1965': 1}, 'doc': {'1906': 1, '1908': 7, '1909': 2, '1912': 4, '1914': 1, '1915': 5, '1916': 9, '1928': 3, '1931': 2, '1932': 1, '1933': 1, '1938': 1, '1941': 2, '1944': 1, '1946': 1, '1947': 1, '1964': 1, '1965': 1}}\n",
      "Line 1100051: {'ngram': 'lumbar radicular pain', 'freq_tot': 910, 'doc_tot': 672, 'freq': {'1983': 60, '1986': 7, '1987': 12, '1991': 11, '1992': 25, '1993': 17, '1994': 7, '1995': 21, '1996': 20, '1998': 9, '2002': 24, '2003': 17, '2005': 36, '2006': 25, '2007': 46, '2008': 68, '2010': 45, '2011': 72, '2012': 53, '2013': 49, '2014': 27, '2015': 44, '2016': 18, '2017': 23, '2018': 50, '2019': 29, '1972': 2, '1989': 17, '1990': 9, '1999': 7, '2004': 22, '2009': 14, '1985': 3, '1988': 4, '1997': 7, '2000': 4, '2001': 6}, 'doc': {'1983': 7, '1986': 7, '1987': 11, '1991': 8, '1992': 23, '1993': 17, '1994': 7, '1995': 21, '1996': 17, '1998': 9, '2002': 24, '2003': 14, '2005': 34, '2006': 24, '2007': 29, '2008': 39, '2010': 33, '2011': 51, '2012': 38, '2013': 33, '2014': 23, '2015': 31, '2016': 16, '2017': 22, '2018': 34, '2019': 19, '1972': 2, '1989': 17, '1990': 9, '1999': 7, '2004': 15, '2009': 9, '1985': 3, '1988': 4, '1997': 5, '2000': 4, '2001': 6}}\n",
      "Line 1100052: {'ngram': 'lumbar radiculopathy', 'freq_tot': 511, 'doc_tot': 346, 'freq': {'1989': 1, '1994': 1, '1995': 5, '1998': 5, '1999': 2, '2001': 2, '2002': 31, '2003': 5, '2004': 5, '2005': 7, '2006': 10, '2007': 14, '2008': 4, '2009': 18, '2010': 12, '2012': 26, '2014': 90, '2016': 52, '2017': 9, '2019': 16, '2015': 121, '1950': 1, '1988': 3, '1996': 1, '2000': 26, '2011': 27, '2013': 11, '2018': 5, '1993': 1}, 'doc': {'1989': 1, '1994': 1, '1995': 5, '1998': 3, '1999': 2, '2001': 2, '2002': 10, '2003': 3, '2004': 5, '2005': 7, '2006': 7, '2007': 9, '2008': 4, '2009': 8, '2010': 8, '2012': 13, '2014': 82, '2016': 24, '2017': 9, '2019': 9, '2015': 99, '1950': 1, '1988': 3, '1996': 1, '2000': 6, '2011': 11, '2013': 7, '2018': 5, '1993': 1}}\n",
      "Line 1100053: {'ngram': 'lumbar radiculopathy update', 'freq_tot': 78, 'doc_tot': 53, 'freq': {'2014': 27, '2015': 51}, 'doc': {'2014': 19, '2015': 34}}\n",
      "Line 1100054: {'ngram': 'lumbar radiograph', 'freq_tot': 103, 'doc_tot': 100, 'freq': {'2014': 18, '2015': 28, '1999': 4, '2000': 5, '2001': 2, '2002': 8, '2003': 4, '2004': 5, '2005': 4, '2007': 2, '2008': 3, '2009': 1, '2011': 1, '2012': 6, '2013': 6, '2016': 4, '2017': 1, '2018': 1}, 'doc': {'2014': 18, '2015': 28, '1999': 4, '2000': 5, '2001': 2, '2002': 8, '2003': 4, '2004': 5, '2005': 3, '2007': 2, '2008': 3, '2009': 1, '2011': 1, '2012': 5, '2013': 6, '2016': 3, '2017': 1, '2018': 1}}\n",
      "Line 1100055: {'ngram': 'lumbar radiograph early', 'freq_tot': 62, 'doc_tot': 59, 'freq': {'1999': 4, '2000': 5, '2001': 2, '2002': 8, '2003': 4, '2004': 5, '2005': 4, '2007': 2, '2008': 3, '2009': 1, '2011': 1, '2012': 6, '2013': 6, '2014': 3, '2015': 2, '2016': 4, '2017': 1, '2018': 1}, 'doc': {'1999': 4, '2000': 5, '2001': 2, '2002': 8, '2003': 4, '2004': 5, '2005': 3, '2007': 2, '2008': 3, '2009': 1, '2011': 1, '2012': 5, '2013': 6, '2014': 3, '2015': 2, '2016': 3, '2017': 1, '2018': 1}}\n",
      "Line 1100056: {'ngram': 'lumbar radiological supervision', 'freq_tot': 152, 'doc_tot': 150, 'freq': {'1980': 4, '1992': 4, '1993': 6, '1995': 24, '1996': 10, '1997': 2, '1998': 3, '1999': 10, '2000': 4, '2001': 2, '2002': 5, '2003': 10, '2004': 10, '2005': 22, '2006': 10, '2007': 6, '2008': 4, '2009': 2, '2010': 4, '2011': 2, '2018': 8}, 'doc': {'1980': 4, '1992': 4, '1993': 6, '1995': 24, '1996': 8, '1997': 2, '1998': 3, '1999': 10, '2000': 4, '2001': 2, '2002': 5, '2003': 10, '2004': 10, '2005': 22, '2006': 10, '2007': 6, '2008': 4, '2009': 2, '2010': 4, '2011': 2, '2018': 8}}\n",
      "Line 1100057: {'ngram': 'lumbar ramisection ganglionectomy', 'freq_tot': 153, 'doc_tot': 140, 'freq': {'1895': 4, '1926': 12, '1927': 11, '1929': 2, '1930': 2, '1932': 1, '1935': 16, '1936': 4, '1937': 3, '1938': 2, '1941': 2, '1946': 4, '1948': 4, '1949': 2, '1952': 4, '1954': 2, '1959': 4, '1963': 4, '1964': 4, '1966': 6, '1970': 6, '1972': 2, '1975': 4, '1979': 2, '1980': 2, '1982': 2, '1983': 2, '1985': 4, '1988': 2, '1989': 4, '1990': 2, '1991': 2, '1992': 2, '1995': 2, '1996': 2, '1998': 2, '2002': 4, '2004': 2, '2006': 2, '2012': 6, '2013': 2, '2014': 2}, 'doc': {'1895': 4, '1926': 11, '1927': 8, '1929': 2, '1930': 2, '1932': 1, '1935': 16, '1936': 4, '1937': 2, '1938': 2, '1941': 2, '1946': 4, '1948': 4, '1949': 2, '1952': 4, '1954': 2, '1959': 2, '1963': 4, '1964': 4, '1966': 4, '1970': 4, '1972': 2, '1975': 2, '1979': 2, '1980': 2, '1982': 2, '1983': 2, '1985': 4, '1988': 2, '1989': 4, '1990': 2, '1991': 2, '1992': 2, '1995': 2, '1996': 2, '1998': 2, '2002': 4, '2004': 2, '2006': 2, '2012': 6, '2013': 2, '2014': 2}}\n",
      "Line 1100058: {'ngram': 'lumbar ramus communicans', 'freq_tot': 41, 'doc_tot': 41, 'freq': {'1905': 3, '1906': 2, '1907': 7, '1909': 2, '1910': 2, '1911': 5, '1912': 1, '1913': 2, '1915': 3, '1916': 1, '1917': 2, '1919': 1, '1922': 2, '1925': 1, '1927': 1, '1928': 1, '1930': 1, '1933': 1, '1936': 2, '1940': 1}, 'doc': {'1905': 3, '1906': 2, '1907': 7, '1909': 2, '1910': 2, '1911': 5, '1912': 1, '1913': 2, '1915': 3, '1916': 1, '1917': 2, '1919': 1, '1922': 2, '1925': 1, '1927': 1, '1928': 1, '1930': 1, '1933': 1, '1936': 2, '1940': 1}}\n",
      "Line 1100059: {'ngram': 'lumbar ramus consist', 'freq_tot': 43, 'doc_tot': 43, 'freq': {'1905': 3, '1906': 2, '1907': 7, '1909': 2, '1910': 2, '1911': 5, '1912': 1, '1913': 3, '1915': 3, '1916': 1, '1917': 2, '1918': 1, '1919': 1, '1922': 2, '1925': 1, '1927': 1, '1928': 1, '1930': 1, '1933': 1, '1936': 2, '1940': 1}, 'doc': {'1905': 3, '1906': 2, '1907': 7, '1909': 2, '1910': 2, '1911': 5, '1912': 1, '1913': 3, '1915': 3, '1916': 1, '1917': 2, '1918': 1, '1919': 1, '1922': 2, '1925': 1, '1927': 1, '1928': 1, '1930': 1, '1933': 1, '1936': 2, '1940': 1}}\n",
      "Line 1100060: {'ngram': 'lumbar ramus posterior sacral ramus', 'freq_tot': 58, 'doc_tot': 58, 'freq': {'1972': 1, '1981': 1, '1987': 1, '1989': 1, '1998': 2, '1999': 2, '2001': 1, '2002': 2, '2003': 1, '2004': 2, '2005': 3, '2006': 4, '2007': 2, '2008': 3, '2009': 2, '2010': 5, '2011': 6, '2012': 2, '2013': 3, '2014': 1, '2015': 4, '2016': 3, '2017': 4, '2018': 1, '2019': 1}, 'doc': {'1972': 1, '1981': 1, '1987': 1, '1989': 1, '1998': 2, '1999': 2, '2001': 1, '2002': 2, '2003': 1, '2004': 2, '2005': 3, '2006': 4, '2007': 2, '2008': 3, '2009': 2, '2010': 5, '2011': 6, '2012': 2, '2013': 3, '2014': 1, '2015': 4, '2016': 3, '2017': 4, '2018': 1, '2019': 1}}\n"
     ]
    }
   ],
   "source": [
    "print_jsonl_lines(\n",
    "    file_path=(\n",
    "        '/vast/edk202/NLP_corpora/Google_Books/20200217/eng/'\n",
    "        '5gram_files/6corpus/5gram-corpus.jsonl.lz4'\n",
    "    ),\n",
    "    start_line=1100050,\n",
    "    end_line=1100060,\n",
    "    parse_json=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hist_w2v_env",
   "language": "python",
   "name": "hist_w2v_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
