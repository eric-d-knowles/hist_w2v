{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c8750c1-c3df-492e-b942-d06b355ae6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "os.chdir('/scratch/edk202/hist_w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f85d0db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755306141.764197 3602869 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755306141.769632 3602869 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1755306141.784247 3602869 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755306141.784261 3602869 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755306141.784262 3602869 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755306141.784264 3602869 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "import rocksdict\n",
    "\n",
    "from ngram_tools.download_and_ingest_to_rocksdb import download_and_ingest_to_rocksdb\n",
    "from utils.rocksdb_post_ingest import open_db_read_optimized, run_manual_compaction\n",
    "from utils.resource_summary import print_resource_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5fff30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling src/ngram_filters.pyx because it changed.\n",
      "[1/1] Cythonizing src/ngram_filters.pyx\n",
      "[1/1] Cythonizing src/ngram_filters.pyx\n",
      "running clean\n",
      "running clean\n",
      "removing 'build/temp.linux-x86_64-cpython-311' (and everything under it)\n",
      "removing 'build/lib.linux-x86_64-cpython-311' (and everything under it)\n",
      "'build/bdist.linux-x86_64' does not exist -- can't clean it\n",
      "'build/scripts-3.11' does not exist -- can't clean it\n",
      "removing 'build'\n",
      "removing 'build/temp.linux-x86_64-cpython-311' (and everything under it)\n",
      "removing 'build/lib.linux-x86_64-cpython-311' (and everything under it)\n",
      "'build/bdist.linux-x86_64' does not exist -- can't clean it\n",
      "'build/scripts-3.11' does not exist -- can't clean it\n",
      "removing 'build'\n",
      "running build_ext\n",
      "running build_ext\n",
      "building 'src.reservoir_sampler' extension\n",
      "creating build/temp.linux-x86_64-cpython-311/src\n",
      "/ext3/miniforge3/envs/hist_w2v/bin/x86_64-conda-linux-gnu-cc -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /ext3/miniforge3/envs/hist_w2v/include -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /ext3/miniforge3/envs/hist_w2v/include -fPIC -I/ext3/miniforge3/envs/hist_w2v/include/python3.11 -c src/reservoir_sampler.c -o build/temp.linux-x86_64-cpython-311/src/reservoir_sampler.o\n",
      "building 'src.reservoir_sampler' extension\n",
      "creating build/temp.linux-x86_64-cpython-311/src\n",
      "/ext3/miniforge3/envs/hist_w2v/bin/x86_64-conda-linux-gnu-cc -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /ext3/miniforge3/envs/hist_w2v/include -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /ext3/miniforge3/envs/hist_w2v/include -fPIC -I/ext3/miniforge3/envs/hist_w2v/include/python3.11 -c src/reservoir_sampler.c -o build/temp.linux-x86_64-cpython-311/src/reservoir_sampler.o\n",
      "creating build/lib.linux-x86_64-cpython-311/src\n",
      "/ext3/miniforge3/envs/hist_w2v/bin/x86_64-conda-linux-gnu-cc -shared -Wl,--allow-shlib-undefined -Wl,-rpath,/ext3/miniforge3/envs/hist_w2v/lib -Wl,-rpath-link,/ext3/miniforge3/envs/hist_w2v/lib -L/ext3/miniforge3/envs/hist_w2v/lib -Wl,--allow-shlib-undefined -Wl,-rpath,/ext3/miniforge3/envs/hist_w2v/lib -Wl,-rpath-link,/ext3/miniforge3/envs/hist_w2v/lib -L/ext3/miniforge3/envs/hist_w2v/lib -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,--allow-shlib-undefined -Wl,-rpath,/ext3/miniforge3/envs/hist_w2v/lib -Wl,-rpath-link,/ext3/miniforge3/envs/hist_w2v/lib -L/ext3/miniforge3/envs/hist_w2v/lib -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /ext3/miniforge3/envs/hist_w2v/include -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /ext3/miniforge3/envs/hist_w2v/include build/temp.linux-x86_64-cpython-311/src/reservoir_sampler.o -o build/lib.linux-x86_64-cpython-311/src/reservoir_sampler.cpython-311-x86_64-linux-gnu.so\n",
      "building 'src.count_db_items' extension\n",
      "/ext3/miniforge3/envs/hist_w2v/bin/x86_64-conda-linux-gnu-cc -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /ext3/miniforge3/envs/hist_w2v/include -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /ext3/miniforge3/envs/hist_w2v/include -fPIC -I/ext3/miniforge3/envs/hist_w2v/include/python3.11 -c src/count_db_items.c -o build/temp.linux-x86_64-cpython-311/src/count_db_items.o\n",
      "creating build/lib.linux-x86_64-cpython-311/src\n",
      "/ext3/miniforge3/envs/hist_w2v/bin/x86_64-conda-linux-gnu-cc -shared -Wl,--allow-shlib-undefined -Wl,-rpath,/ext3/miniforge3/envs/hist_w2v/lib -Wl,-rpath-link,/ext3/miniforge3/envs/hist_w2v/lib -L/ext3/miniforge3/envs/hist_w2v/lib -Wl,--allow-shlib-undefined -Wl,-rpath,/ext3/miniforge3/envs/hist_w2v/lib -Wl,-rpath-link,/ext3/miniforge3/envs/hist_w2v/lib -L/ext3/miniforge3/envs/hist_w2v/lib -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,--allow-shlib-undefined -Wl,-rpath,/ext3/miniforge3/envs/hist_w2v/lib -Wl,-rpath-link,/ext3/miniforge3/envs/hist_w2v/lib -L/ext3/miniforge3/envs/hist_w2v/lib -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /ext3/miniforge3/envs/hist_w2v/include -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /ext3/miniforge3/envs/hist_w2v/include build/temp.linux-x86_64-cpython-311/src/reservoir_sampler.o -o build/lib.linux-x86_64-cpython-311/src/reservoir_sampler.cpython-311-x86_64-linux-gnu.so\n",
      "building 'src.count_db_items' extension\n",
      "/ext3/miniforge3/envs/hist_w2v/bin/x86_64-conda-linux-gnu-cc -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /ext3/miniforge3/envs/hist_w2v/include -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /ext3/miniforge3/envs/hist_w2v/include -fPIC -I/ext3/miniforge3/envs/hist_w2v/include/python3.11 -c src/count_db_items.c -o build/temp.linux-x86_64-cpython-311/src/count_db_items.o\n",
      "/ext3/miniforge3/envs/hist_w2v/bin/x86_64-conda-linux-gnu-cc -shared -Wl,--allow-shlib-undefined -Wl,-rpath,/ext3/miniforge3/envs/hist_w2v/lib -Wl,-rpath-link,/ext3/miniforge3/envs/hist_w2v/lib -L/ext3/miniforge3/envs/hist_w2v/lib -Wl,--allow-shlib-undefined -Wl,-rpath,/ext3/miniforge3/envs/hist_w2v/lib -Wl,-rpath-link,/ext3/miniforge3/envs/hist_w2v/lib -L/ext3/miniforge3/envs/hist_w2v/lib -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,--allow-shlib-undefined -Wl,-rpath,/ext3/miniforge3/envs/hist_w2v/lib -Wl,-rpath-link,/ext3/miniforge3/envs/hist_w2v/lib -L/ext3/miniforge3/envs/hist_w2v/lib -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /ext3/miniforge3/envs/hist_w2v/include -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /ext3/miniforge3/envs/hist_w2v/include build/temp.linux-x86_64-cpython-311/src/count_db_items.o -o build/lib.linux-x86_64-cpython-311/src/count_db_items.cpython-311-x86_64-linux-gnu.so\n",
      "/ext3/miniforge3/envs/hist_w2v/bin/x86_64-conda-linux-gnu-cc -shared -Wl,--allow-shlib-undefined -Wl,-rpath,/ext3/miniforge3/envs/hist_w2v/lib -Wl,-rpath-link,/ext3/miniforge3/envs/hist_w2v/lib -L/ext3/miniforge3/envs/hist_w2v/lib -Wl,--allow-shlib-undefined -Wl,-rpath,/ext3/miniforge3/envs/hist_w2v/lib -Wl,-rpath-link,/ext3/miniforge3/envs/hist_w2v/lib -L/ext3/miniforge3/envs/hist_w2v/lib -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,--allow-shlib-undefined -Wl,-rpath,/ext3/miniforge3/envs/hist_w2v/lib -Wl,-rpath-link,/ext3/miniforge3/envs/hist_w2v/lib -L/ext3/miniforge3/envs/hist_w2v/lib -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /ext3/miniforge3/envs/hist_w2v/include -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /ext3/miniforge3/envs/hist_w2v/include build/temp.linux-x86_64-cpython-311/src/count_db_items.o -o build/lib.linux-x86_64-cpython-311/src/count_db_items.cpython-311-x86_64-linux-gnu.so\n",
      "building 'src.ngram_filters' extension\n",
      "/ext3/miniforge3/envs/hist_w2v/bin/x86_64-conda-linux-gnu-cc -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /ext3/miniforge3/envs/hist_w2v/include -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /ext3/miniforge3/envs/hist_w2v/include -fPIC -I/ext3/miniforge3/envs/hist_w2v/include/python3.11 -c src/ngram_filters.c -o build/temp.linux-x86_64-cpython-311/src/ngram_filters.o\n",
      "building 'src.ngram_filters' extension\n",
      "/ext3/miniforge3/envs/hist_w2v/bin/x86_64-conda-linux-gnu-cc -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /ext3/miniforge3/envs/hist_w2v/include -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /ext3/miniforge3/envs/hist_w2v/include -fPIC -I/ext3/miniforge3/envs/hist_w2v/include/python3.11 -c src/ngram_filters.c -o build/temp.linux-x86_64-cpython-311/src/ngram_filters.o\n",
      "/ext3/miniforge3/envs/hist_w2v/bin/x86_64-conda-linux-gnu-cc -shared -Wl,--allow-shlib-undefined -Wl,-rpath,/ext3/miniforge3/envs/hist_w2v/lib -Wl,-rpath-link,/ext3/miniforge3/envs/hist_w2v/lib -L/ext3/miniforge3/envs/hist_w2v/lib -Wl,--allow-shlib-undefined -Wl,-rpath,/ext3/miniforge3/envs/hist_w2v/lib -Wl,-rpath-link,/ext3/miniforge3/envs/hist_w2v/lib -L/ext3/miniforge3/envs/hist_w2v/lib -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,--allow-shlib-undefined -Wl,-rpath,/ext3/miniforge3/envs/hist_w2v/lib -Wl,-rpath-link,/ext3/miniforge3/envs/hist_w2v/lib -L/ext3/miniforge3/envs/hist_w2v/lib -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /ext3/miniforge3/envs/hist_w2v/include -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /ext3/miniforge3/envs/hist_w2v/include build/temp.linux-x86_64-cpython-311/src/ngram_filters.o -o build/lib.linux-x86_64-cpython-311/src/ngram_filters.cpython-311-x86_64-linux-gnu.so\n",
      "/ext3/miniforge3/envs/hist_w2v/bin/x86_64-conda-linux-gnu-cc -shared -Wl,--allow-shlib-undefined -Wl,-rpath,/ext3/miniforge3/envs/hist_w2v/lib -Wl,-rpath-link,/ext3/miniforge3/envs/hist_w2v/lib -L/ext3/miniforge3/envs/hist_w2v/lib -Wl,--allow-shlib-undefined -Wl,-rpath,/ext3/miniforge3/envs/hist_w2v/lib -Wl,-rpath-link,/ext3/miniforge3/envs/hist_w2v/lib -L/ext3/miniforge3/envs/hist_w2v/lib -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,--allow-shlib-undefined -Wl,-rpath,/ext3/miniforge3/envs/hist_w2v/lib -Wl,-rpath-link,/ext3/miniforge3/envs/hist_w2v/lib -L/ext3/miniforge3/envs/hist_w2v/lib -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /ext3/miniforge3/envs/hist_w2v/include -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /ext3/miniforge3/envs/hist_w2v/include build/temp.linux-x86_64-cpython-311/src/ngram_filters.o -o build/lib.linux-x86_64-cpython-311/src/ngram_filters.cpython-311-x86_64-linux-gnu.so\n",
      "copying build/lib.linux-x86_64-cpython-311/src/reservoir_sampler.cpython-311-x86_64-linux-gnu.so -> src\n",
      "copying build/lib.linux-x86_64-cpython-311/src/count_db_items.cpython-311-x86_64-linux-gnu.so -> src\n",
      "copying build/lib.linux-x86_64-cpython-311/src/ngram_filters.cpython-311-x86_64-linux-gnu.so -> src\n",
      "copying build/lib.linux-x86_64-cpython-311/src/reservoir_sampler.cpython-311-x86_64-linux-gnu.so -> src\n",
      "copying build/lib.linux-x86_64-cpython-311/src/count_db_items.cpython-311-x86_64-linux-gnu.so -> src\n",
      "copying build/lib.linux-x86_64-cpython-311/src/ngram_filters.cpython-311-x86_64-linux-gnu.so -> src\n"
     ]
    }
   ],
   "source": [
    "!python setup.py clean --all\n",
    "!python setup.py build_ext --inplace --force\n",
    "\n",
    "# Reload the optimized modules\n",
    "from src.reservoir_sampler import reservoir_sampling\n",
    "from src.count_db_items import count_db_items\n",
    "from src.ngram_filters import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e91dc439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<pre>SYSTEM RESOURCE SUMMARY\n",
       "=============================================\n",
       "Hostname: cm050.hpc.nyu.edu\n",
       "\n",
       "Job Allocation:\n",
       "   CPUs: 40\n",
       "   Memory: 293.0 GB\n",
       "   Partition: short\n",
       "   Job ID: 65020982\n",
       "   Node list: cm050\n",
       "\n",
       "Physical GPU Hardware:\n",
       "   No physical GPUs allocated to this job\n",
       "\n",
       "TensorFlow GPU Recognition:\n",
       "   TensorFlow can access 0 GPU(s)\n",
       "   Built with CUDA support: True\n",
       "=============================================</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_resource_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bf72e9",
   "metadata": {},
   "source": [
    "# Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f582cab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_dir = \"/vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/\"\n",
    "db_path = os.path.join(proj_dir, \"5grams.db\")\n",
    "\n",
    "download_and_ingest_to_rocksdb(\n",
    "    ngram_size=5,\n",
    "    ngram_type='tagged',\n",
    "    repo_release_id=\"20200217\",\n",
    "    repo_corpus_id=\"eng\",\n",
    "    db_path=db_path,\n",
    "    #file_range=(0, 999),\n",
    "    workers=16,\n",
    "    write_batch_size=10000000,\n",
    "    use_threads=False,\n",
    "    overwrite=False,\n",
    "    random_seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1248777",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_dir = \"/vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/\"\n",
    "db_path = os.path.join(proj_dir, \"5grams.db\")\n",
    "\n",
    "run_manual_compaction(db_path, optimization=\"read\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebff105",
   "metadata": {},
   "source": [
    "# Inspect Database Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad1f2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = \"/vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams.db\"\n",
    "db = open_db_read_optimized(db_path)\n",
    "\n",
    "import struct\n",
    "\n",
    "# Request (key, value) tuples now that value payload is packed binary\n",
    "sample = reservoir_sampling(\n",
    "    db,\n",
    "    sample_size=10,\n",
    "    key_type=\"string\",\n",
    "    progress_interval=100_000_000,\n",
    "    max_items=2_500_000_000,\n",
    "    return_keys=True,\n",
    ")\n",
    "\n",
    "print(\"\\nSAMPLE\")\n",
    "print(\"-\" * 60)\n",
    "for i, item in enumerate(sample, 1):\n",
    "    key, value = item\n",
    "    freq_tuples = list(struct.iter_unpack('<III', value))\n",
    "    print(f\"{i}. {key}\")\n",
    "    for j, (year, match_count, volume_count) in enumerate(freq_tuples, 1):\n",
    "        print(f\"    {j}: year={year}, match_count={match_count}, volume_count={volume_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe73395",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams.db/LOCK\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "proj_dir = \"/vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/\"\n",
    "db_path = os.path.join(proj_dir, \"5grams.db\")\n",
    "\n",
    "db = open_db_read_optimized(db_path)\n",
    "\n",
    "keys = []\n",
    "for i, (key, value) in enumerate(db.items()):\n",
    "    if i >= 10:\n",
    "        break\n",
    "    keys.append(key)\n",
    "\n",
    "for key in keys:\n",
    "    print(f\"Key:         {key}\")\n",
    "    key = tokenize(key)\n",
    "    print(f\"Tokenized:   {key}\")\n",
    "    key = split(key)\n",
    "    print(f\"Split:       {key}\")\n",
    "    key = lower(key)\n",
    "    print(f\"Lowercased:  {key}\")\n",
    "    key = alpha(key)\n",
    "    print(f\"Alpha:       {key}\")\n",
    "    key = shorts(key, 3)\n",
    "    print(f\"Short:       {key}\")\n",
    "    key = stops(key, stopwords)\n",
    "    print(f\"Stopwords:   {key}\")\n",
    "    key = lemmas(key, lemmatizer)\n",
    "    print(f\"Lemmatized:  {key}\")\n",
    "    key = rejoin(key)\n",
    "    print(f\"New Key:     {key}\\n\")\n",
    "\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3b5398",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams.db/LOCK\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "proj_dir = \"/vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/\"\n",
    "db_path = os.path.join(proj_dir, \"5grams.db\")\n",
    "\n",
    "db = open_db_read_optimized(db_path)\n",
    "\n",
    "tag_map = {\n",
    "    \"NOUN\": \"n\",\n",
    "    \"VERB\": \"v\",\n",
    "    \"ADJ\": \"a\",\n",
    "    \"ADV\": \"r\"\n",
    "}\n",
    "\n",
    "keys = []\n",
    "for i, (key, value) in enumerate(db.items()):\n",
    "    if i >= 10:\n",
    "        break\n",
    "    keys.append(key)\n",
    "\n",
    "for key in keys:\n",
    "    print(f\"Original:   {key}\")\n",
    "    t0 = time.time()\n",
    "    t1 = time.time(); key = tokenize(key); t2 = time.time()\n",
    "    t3 = time.time(); key = split(key); t4 = time.time()\n",
    "    t5 = time.time(); key = lower(key); t6 = time.time()\n",
    "    t7 = time.time(); key = alpha(key); t8 = time.time()\n",
    "    t9 = time.time(); key = shorts(key, 3); t10 = time.time()\n",
    "    t11 = time.time(); key = stops(key, stopwords); t12 = time.time()\n",
    "    t13 = time.time(); key = lemmas(key, lemmatizer, tag_map); t14 = time.time()\n",
    "    t15 = time.time(); key = rejoin(key); t16 = time.time()\n",
    "    print(f\"Final:      {key}\\n\")\n",
    "    print(\"Step timings (microseconds):\")\n",
    "    print(f\"  Tokenize:    {(t2-t1)*1e6:.1f}\")\n",
    "    print(f\"  Split:       {(t4-t3)*1e6:.1f}\")\n",
    "    print(f\"  Lowercase:   {(t6-t5)*1e6:.1f}\")\n",
    "    print(f\"  Alpha:       {(t8-t7)*1e6:.1f}\")\n",
    "    print(f\"  Short:       {(t10-t9)*1e6:.1f}\")\n",
    "    print(f\"  Stopwords:   {(t12-t11)*1e6:.1f}\")\n",
    "    print(f\"  Lemmatize:   {(t14-t13)*1e6:.1f}\")\n",
    "    print(f\"  Rejoin:      {(t16-t15)*1e6:.1f}\")\n",
    "    print(f\"  Total:       {(t16-t0)*1e6:.1f}\\n\")\n",
    "\n",
    "db.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3278b902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"_. A_DET secured_ADJ party_NOUN may_VERB\t(1931, 1, 1)\t(1960, 3, 3)\t(1962, 2, 2)\t(1963, 3, 2)\t(1964, 1, 1)\t(1967, 1, 1)\t(1968, 4, 4)\t(1970, 1, 1)\t(1971, 6, 4)\t(1973, 2, 2)\t(1975, 1, 1)\t(1976, 1, 1)\t(1977, 2, 1)\t(1981, 7, 3)\t(1982, 2, 1)\t(1984, 7, 7)\t(1985, 7, 7)\t(1986, 4, 2)\t(1987, 6, 6)\t(1988, 1, 1)\t(1989, 1, 1)\t(1990, 1, 1)\t(1991, 1, 1)\t(1992, 19, 11)\t(1993, 13, 13)\t(1994, 14, 10)\t(1995, 3, 2)\t(1996, 4, 2)\t(1997, 3, 3)\t(1998, 4, 3)\t(1999, 4, 3)\t(2000, 6, 3)\t(2001, 9, 8)\t(2002, 4, 4)\t(2003, 1, 1)\t(2004, 1, 1)\t(2005, 2, 2)\t(2006, 1, 1)\t(2011, 2, 2)\t(2012, 1, 1)\t(2013, 4, 3)\n",
      "\"_. A_DET secured_ADJ party_NOUN may_VERB -> <UNK> <UNK> secured party may\n",
      "\"_. A_DET secured_ADJ party_NOUN must_VERB -> <UNK> <UNK> secured party must\n",
      "\"_. A_DET secured_ADJ party_NOUN of_ADP -> <UNK> <UNK> secured party <UNK>\n",
      "\"_. A_DET secured_ADJ party_NOUN who_PRON -> <UNK> <UNK> secured party <UNK>\n",
      "\"_. A_DET securities_NOUN dealer_NOUN occupies_VERB -> <UNK> <UNK> security dealer occupy\n",
      "\"_. A_DET security_NOUN agreement_NOUN is_VERB -> <UNK> <UNK> security agreement <UNK>\n",
      "\"_. A_DET security_NOUN agreement_NOUN may_VERB -> <UNK> <UNK> security agreement may\n",
      "\"_. A_DET security_NOUN architecture_NOUN for_ADP -> <UNK> <UNK> security architecture <UNK>\n",
      "\"_. A_DET security_NOUN for_ADP the_DET -> <UNK> <UNK> security <UNK> <UNK>\n",
      "\"_. A_DET security_NOUN guard_NOUN ,_. -> <UNK> <UNK> security guard <UNK>\n"
     ]
    }
   ],
   "source": [
    "!rm /vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams.db/LOCK\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import struct\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "proj_dir = \"/vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/\"\n",
    "db_path = os.path.join(proj_dir, \"5grams.db\")\n",
    "\n",
    "db = open_db_read_optimized(db_path)\n",
    "\n",
    "key, value = next(iter(db.items()))\n",
    "freq_tuples = list(struct.iter_unpack('<III', value))\n",
    "freq_tuples = '\\t'.join(str(tup) for tup in freq_tuples)\n",
    "print(f\"{key}\\t{freq_tuples}\")\n",
    "\n",
    "for key in keys:\n",
    "    new_key = process(\n",
    "        key,\n",
    "        opt_lower=True,\n",
    "        opt_alpha=True,\n",
    "        opt_shorts=True,\n",
    "        opt_stops=True,\n",
    "        opt_lemmas=True,\n",
    "        stop_set=stopwords,\n",
    "        lemma_gen=lemmatizer,\n",
    "    )\n",
    "    print(key + \" -> \" + new_key)\n",
    "\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e05314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import struct\n",
    "import rocksdict\n",
    "\n",
    "# Paths\n",
    "orig_db_dir = \"/vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams.db\"\n",
    "copy_db_dir = \"/vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams.db.processed\"\n",
    "\n",
    "# Copy the DB directory\n",
    "if os.path.exists(copy_db_dir):\n",
    "    shutil.rmtree(copy_db_dir)\n",
    "shutil.copytree(orig_db_dir, copy_db_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2bdf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import struct\n",
    "import rocksdict\n",
    "\n",
    "# Paths\n",
    "orig_db_dir = \"/vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams.db\"\n",
    "copy_db_dir = \"/vast/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams.db.processed\"\n",
    "\n",
    "# Copy the DB directory\n",
    "if os.path.exists(copy_db_dir):\n",
    "    shutil.rmtree(copy_db_dir)\n",
    "shutil.copytree(orig_db_dir, copy_db_dir)\n",
    "\n",
    "# Use the copy for processing\n",
    "db = rocksdict.Rdict(copy_db_dir, read_only=False)\n",
    "\n",
    "def freq_merge(existing, new):\n",
    "    freq_dict = {}\n",
    "    for tup in struct.iter_unpack('<III', existing + new):\n",
    "        year, match_count, volume_count = tup\n",
    "        if year not in freq_dict:\n",
    "            freq_dict[year] = [match_count, volume_count]\n",
    "        else:\n",
    "            freq_dict[year][0] += match_count\n",
    "            freq_dict[year][1] += volume_count\n",
    "    agg_tuples = [(year, mc, vc) for year, (mc, vc) in freq_dict.items()]\n",
    "    return struct.pack(f'<{len(agg_tuples)*3}I', *(x for tup in agg_tuples for x in tup))\n",
    "\n",
    "db.set_merge_operator(freq_merge)\n",
    "\n",
    "for key, value in db.items():\n",
    "    freq_tuples = list(struct.iter_unpack('<III', value))\n",
    "    new_key = process(\n",
    "        key,\n",
    "        opt_lower=True,\n",
    "        opt_alpha=True,\n",
    "        opt_shorts=True,\n",
    "        opt_stops=True,\n",
    "        opt_lemmas=True,\n",
    "        stop_set=stopwords,\n",
    "        lemma_gen=lemmatizer,\n",
    "    )\n",
    "    packed = struct.pack(f'<{len(freq_tuples)*3}I', *(x for tup in freq_tuples for x in tup))\n",
    "    db.merge(new_key, packed)\n",
    "    del db[key]\n",
    "\n",
    "db.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Remote kernel: hist_w2v",
   "language": "python",
   "name": "hist_w2v"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
