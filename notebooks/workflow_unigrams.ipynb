{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b50e6a0e-e26c-45d6-977b-15aad5e7477f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8ffefae-714c-4ff8-b207-230d6d0f63f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngram_tools.download_ngrams import download_ngram_files\n",
    "from ngram_tools.convert_to_jsonl import convert_to_jsonl_files\n",
    "from ngram_tools.lowercase_ngrams import lowercase_ngrams\n",
    "from ngram_tools.lemmatize_ngrams import lemmatize_ngrams\n",
    "from ngram_tools.filter_ngrams import filter_ngrams\n",
    "from ngram_tools.sort_ngrams import sort_ngrams\n",
    "from ngram_tools.consolidate_ngrams import consolidate_duplicate_ngrams\n",
    "from ngram_tools.index_and_create_vocab import index_and_create_vocab_files\n",
    "from ngram_tools.helpers.verify_sort import check_file_sorted\n",
    "from ngram_tools.helpers.print_jsonl_lines import print_jsonl_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cdb7cc-0186-4bbc-bc10-22b433eca869",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# **Process Unigrams to Generate a Vocabulary File**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecddde54-96c0-4461-a0f7-6ef2bbecd065",
   "metadata": {},
   "source": [
    "## **Goal**: Make a list of the _n_ most common unigrams for later use filtering multigrams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "195fa408-af40-4c69-b360-018748472598",
   "metadata": {},
   "source": [
    "This workflow is resource-intensive and is probably only practical when run on a computing cluster. On my university's High Performance Computing (HPC) cluster, I request the maximum 14 cores (48 logical processors) and 128G of memory and use a 2T fast-I/O NVMe SSD filespace—and I still run up against time and resource limits. I've designed the code to be efficient, although further optimization is surely possible.\n",
    "\n",
    "The code affords options to conserve resources. Throughout the workflow you can specify `compress=True`, which tells a script to compress its output files. In my experience, there is little downside to using LZ4 compression, since it's very fast and cuts file sizes by about half. Downstream modules will see the `.lz4` extensions and handle the files accordingly. If you know your workflow runs correctly and wish to further conserve space, you can specify `delete_input=True` for many of the scripts; this will delete the source files for a given step once it is complete. The scripts are fairly memory-efficient—with the exception of `sort_ngrams` and `index_and_create_vocab_files`, which sort multiple files in memory at once. When processing multigrams, I've found that allocating more than ~10 workers in these scripts leads to memory exhaustion (with 128G!) and slow processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97fea36-f682-4a1c-aedc-d905ba30c001",
   "metadata": {},
   "source": [
    "### Download unigrams\n",
    "Here, I'm using the `download_ngrams` module to fetch unigrams appended with part-of-speech (POS) tags (e.g., `_VERB`). Although you can specify `ngram_type='untagged'`, POS tags are necessary to lemmatize the tokens. Specify the number of parallel processes you wish to use by setting `workers` (the default is all available processors). Note the `repo_release_id` and `repo_corpus_id` parameters; these tell the module which ngram corpus and release to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25bd2f2a-e4fb-4af0-bd94-71ef7c5dcdb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mStart Time:                2025-04-07 16:13:17.441931\n",
      "\u001b[0m\n",
      "\u001b[4mDownload Info\u001b[0m\n",
      "Ngram repository:          https://storage.googleapis.com/books/ngrams/books/20200217/eng-fiction/eng-fiction-1-ngrams_exports.html\n",
      "Output directory:          /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/1gram_files/1download\n",
      "File index range:          0 to 0\n",
      "File URLs available:       1\n",
      "File URLs to use:          1\n",
      "First file to get:         https://storage.googleapis.com/books/ngrams/books/20200217/eng-fiction/1-00000-of-00001.gz\n",
      "Last file to get:          https://storage.googleapis.com/books/ngrams/books/20200217/eng-fiction/1-00000-of-00001.gz\n",
      "Ngram size:                1\n",
      "Ngram type:                tagged\n",
      "Number of workers:         48\n",
      "Compress saved files:      True\n",
      "Overwrite existing files:  True\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b715151d9e2402782ab805cb11ff9e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0/1 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      "End Time:                  2025-04-07 16:13:50.611621\u001b[0m\n",
      "\u001b[31mTotal runtime:             0:00:33.169690\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "download_ngram_files(\n",
    "    ngram_size=1,\n",
    "    ngram_type='tagged',\n",
    "    repo_release_id='20200217',\n",
    "    repo_corpus_id='eng-fiction',\n",
    "    proj_dir='/vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction',\n",
    "    compress=True,\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799d4fd8-2fe9-4fe3-8a37-20f0e1b81eef",
   "metadata": {},
   "source": [
    "### Convert files from TXT to JSONL\n",
    "This module converts the original unigram files' text data to a more flexible JSON Lines (JSONL) format. Although this increases storage demands, it makes downstream processing more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccf070c7-54bd-44b0-aff7-e86a3b36291e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mStart Time:                2025-04-07 16:13:54.730968\n",
      "\u001b[0m\n",
      "\u001b[4mConversion Info\u001b[0m\n",
      "Input directory:           /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/1gram_files/1download\n",
      "Output directory:          /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/1gram_files/2convert\n",
      "File index range:          0 to 0\n",
      "Files available:           1\n",
      "Files to use:              1\n",
      "First file to get:         /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/1gram_files/1download/1-00000-of-00001.txt.lz4\n",
      "Last file to get:          /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/1gram_files/1download/1-00000-of-00001.txt.lz4\n",
      "Ngram size:                1\n",
      "Ngram type:                tagged\n",
      "Number of workers:         48\n",
      "Compress output files:     True\n",
      "Overwrite existing files:  True\n",
      "Delete input directory:    True\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c817ab9b16a4d129fe7dbdfde7c8dca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting:   0%|          | 0/1 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      "End Time:                  2025-04-07 16:15:48.691505\u001b[0m\n",
      "\u001b[31mTotal runtime:             0:01:53.960537\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "convert_to_jsonl_files(\n",
    "    ngram_size=1,\n",
    "    ngram_type='tagged',\n",
    "    proj_dir='/vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction',\n",
    "    compress=True,\n",
    "    overwrite=True,\n",
    "    delete_input=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ad8638-1ed1-4ce6-b864-70e17635a3a8",
   "metadata": {},
   "source": [
    "### Make unigrams all lowercase\n",
    "This module lowercases all characters in the unigrams. Most use cases benefit from this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cc700cf-dba7-40d6-b5f0-e98723af2e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mStart Time:                2025-04-07 16:17:03.758827\n",
      "\u001b[0m\n",
      "\u001b[4mLowercasing Info\u001b[0m\n",
      "Input directory:           /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/1gram_files/2convert\n",
      "Output directory:          /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/1gram_files/3lowercase\n",
      "File index range:          0 to 0\n",
      "Files available:           1\n",
      "Files to use:              1\n",
      "First file to get:         /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/1gram_files/2convert/1-00000-of-00001.jsonl.lz4\n",
      "Last file to get:          /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/1gram_files/2convert/1-00000-of-00001.jsonl.lz4\n",
      "Ngram size:                1\n",
      "Number of workers:         48\n",
      "Compress output files:     True\n",
      "Overwrite existing files:  True\n",
      "Delete input directory:    True\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e5a9ece65354f71834b3bfee4a9993b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lowercasing:   0%|          | 0/1 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      "End Time:                  2025-04-07 16:17:55.339178\u001b[0m\n",
      "\u001b[31mTotal runtime:             0:00:51.580351\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "lowercase_ngrams(\n",
    "    ngram_size=1,\n",
    "    proj_dir='/vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction',\n",
    "    compress=True,\n",
    "    overwrite=True,\n",
    "    delete_input=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8426e48d-76d8-42ff-9e95-7d9c44bdc331",
   "metadata": {},
   "source": [
    "### Lemmatize the unigrams\n",
    "This module lemmatizes the unigrams—that is, reduces them to their base forms. This is desirable for most use cases. Example: `people_NOUN` (\"the people of this land\") will be converted to `person` in the output; `people_VERB` (\"to people this land\") will not. My code uses the [NLTK Lemmatizer](https://www.nltk.org/api/nltk.stem.WordNetLemmatizer.html?highlight=wordnet), which requires requires POS-tagged unigrams. The tags are discarded after lemmatization as they're no longer useful, saving space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82eb3d13-9d51-4ea2-8708-9c3eadb3feae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mStart Time:                2025-04-07 16:18:05.282307\n",
      "\u001b[0m\n",
      "\u001b[4mLemmatizing Info\u001b[0m\n",
      "Input directory:           /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/1gram_files/3lowercase\n",
      "Output directory:          /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/1gram_files/4lemmatize\n",
      "File index range:          0 to 0\n",
      "Files available:           1\n",
      "Files to use:              1\n",
      "First file to get:         /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/1gram_files/3lowercase/1-00000-of-00001.jsonl.lz4\n",
      "Last file to get:          /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/1gram_files/3lowercase/1-00000-of-00001.jsonl.lz4\n",
      "Ngram size:                1\n",
      "Number of workers:         48\n",
      "Compress output files:     True\n",
      "Overwrite existing files:  True\n",
      "Delete input directory:    True\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd28c64838bd42328765b9c292a48992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatizing:   0%|          | 0/1 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      "End Time:                  2025-04-07 16:19:10.595392\u001b[0m\n",
      "\u001b[31mTotal runtime:             0:01:05.313085\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "lemmatize_ngrams(\n",
    "    ngram_size=1,\n",
    "    proj_dir='/vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction',\n",
    "    compress=True,\n",
    "    overwrite=True,\n",
    "    delete_input=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd52fad3-d372-4613-9c36-b153ca004976",
   "metadata": {},
   "source": [
    "### Filter the unigrams\n",
    "This module removes tokens that provide little information about words' semantic context: those that contain numerals (`numerals=True`), nonalphabetic characters (`nonalpha=True`), stopwords (high-frequency, low information tokens like \"the\" and \"into\"; `stops=True`), or short words (those below a certain user-specified character count; here, `min_token_length=3`).\n",
    "\n",
    "You can also specify `min_tokens`—the minumum length of a retained ngram after filtering its tokens. This is mainly intended for use when processing multigrams. However, it's still good to specify `min_tokens=1` for unigrams, as it completely discards the data for any unigram violating our criteria. Empty output files will be deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68104701-700e-4b0d-8b3e-b69127a75ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mStart Time:                   2025-04-07 16:19:45.546036\n",
      "\u001b[0m\n",
      "\u001b[4mFiltering Info\u001b[0m\n",
      "Input directory:              /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/1gram_files/4lemmatize\n",
      "Output directory:             /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/1gram_files/5filter\n",
      "File index range:             0 to 0\n",
      "Files available:              1\n",
      "Files to use:                 1\n",
      "First file to get:            /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/1gram_files/4lemmatize/1-00000-of-00001.jsonl.lz4\n",
      "Last file to get:             /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/1gram_files/4lemmatize/1-00000-of-00001.jsonl.lz4\n",
      "Ngram size:                   1\n",
      "Number of workers:            48\n",
      "Compress output files:        True\n",
      "Overwrite existing files:     True\n",
      "Delete input directory:       True\n",
      "\n",
      "\u001b[4mFiltering Options\u001b[0m\n",
      "Drop stopwords:               True\n",
      "Drop tokens under:            3 chars\n",
      "Drop tokens with numerals:    True\n",
      "Drop non-alphabetic:          True\n",
      "Drop ngrams under:            1 token(s)\n",
      "Replace tokens:               True\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f2f47ee6c18428a951487058fe32978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering:   0%|          | 0/1 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[4mFiltering Results (Dropped)\u001b[0m\n",
      "Stopword tokens:              2857 \n",
      "Short-word tokens:            9544 \n",
      "Tokens with numerals:         239217 \n",
      "Tokens with non-alpha chars:  162326\n",
      "Out-of-vocab tokens:          0\n",
      "Entire ngrams:                413944 \n",
      "\u001b[31m\n",
      "End Time:                  2025-04-07 16:20:41.832845\u001b[0m\n",
      "\u001b[31mTotal runtime:             0:00:56.286809\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "filter_ngrams(\n",
    "    ngram_size=1,\n",
    "    proj_dir='/vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction',\n",
    "    numerals=True,\n",
    "    nonalpha=True,\n",
    "    stops=True,\n",
    "    min_token_length=3,\n",
    "    min_tokens=1,\n",
    "    compress=True,\n",
    "    overwrite=True,\n",
    "    replace_unk=True,\n",
    "    delete_input=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffd5722-7816-4c68-89a4-6f824ccb3e1a",
   "metadata": {},
   "source": [
    "### Sort and combine the unigram files\n",
    "This modules creates a single, fully-sorted unigram file out of the filtered files. This is crucial for the next step (ngram consolidation; see below).   \n",
    "\n",
    "Sorting a giant file is a resource-hungry process and I've tried to implement an efficient approach that leverages parallelism: We first sort the filtered files in parallel using Python's standard sorting algorithm [Timsort](https://en.wikipedia.org/wiki/Timsort); then, we incrementally [heapsort](https://en.wikipedia.org/wiki/Heapsort) the files in parallel until we get down to 2 files. Finally, we heapsort the final 2 files (necessarily using one processor) to arrive at a single combined and sorted unigram file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52e80618-397f-4051-b064-a8b689e77c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mStart Time:                2025-04-07 16:21:30.422355\n",
      "\u001b[0m\n",
      "\u001b[4mSort Info\u001b[0m\n",
      "Input directory:           /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/1gram_files/5filter\n",
      "Sorted directory:          /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/1gram_files/temp\n",
      "Temp directory:            /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/1gram_files/tmp\n",
      "Merged file:               /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/1gram_files/6corpus/1gram-merged.jsonl.lz4\n",
      "Files available:           1\n",
      "First file to get:         /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/1gram_files/5filter/1-00000-of-00001.jsonl.lz4\n",
      "Last file to get:          /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/1gram_files/5filter/1-00000-of-00001.jsonl.lz4\n",
      "Files to use:              1\n",
      "Ngram size:                1\n",
      "Number of workers:         12\n",
      "Compress output files:     True\n",
      "Overwrite existing files:  True\n",
      "Sort key:                  ngram\n",
      "Sort order:                ascending\n",
      "Heap-merge start iter:     1\n",
      "Heap-merge end iter:       None\n",
      "Deleted sorted files:      True\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a291be49a8b84f84aac317a8da3cd77c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sorting:   0%|          | 0/1 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging complete. Final file: /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/1gram_files/6corpus/1gram-merged.jsonl.lz4\n",
      "\u001b[31m\n",
      "End Time:                  2025-04-07 16:22:34.028715\u001b[0m\n",
      "\u001b[31mTotal runtime:             0:01:03.606360\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "sort_ngrams(\n",
    "    ngram_size=1,\n",
    "    proj_dir='/vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction',\n",
    "    workers=12,\n",
    "    sort_key='ngram',\n",
    "    compress=True,\n",
    "    overwrite=True,\n",
    "    sort_order='ascending',\n",
    "    delete_input=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc33c06-ca7f-4d99-9949-7520ef0e0c98",
   "metadata": {},
   "source": [
    "### Verify sort [OPTIONAL]\n",
    "If we want, we can verify that the output file is correctly sorted. Bear in mind that you need to specify the file path manually here; be sure to use the right file extension based on whether `sort_ngrams` was run with `compress=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2cfd12d0-5dfc-482e-a145-0306b4e01ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines: 2075511line [00:25, 81017.24line/s]\n",
      "\n",
      "The file is sorted.\n"
     ]
    }
   ],
   "source": [
    "check_file_sorted(\n",
    "    input_file=(\n",
    "        '/vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/'\n",
    "        '1gram_files/6corpus/1gram-merged.jsonl.lz4'\n",
    "    ),\n",
    "    field=\"ngram\",\n",
    "    sort_order=\"ascending\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353a8ec7-38b8-4d5d-8356-1af9f8e26bf9",
   "metadata": {},
   "source": [
    "### Consolidate duplicate unigrams\n",
    "This module consolidates the sorted unigram file. Lowercasing and lemmatizing produce duplicate unigrams. Now that the file is sorted, we can scan through it and consolidate consecutive idential duplicates. This involves summing their overall and yearly frequencies and document counts. It also leads to a much smaller file.\n",
    "\n",
    "`[Runtime with compression:  0:07:33.662163`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8609476-7636-43c6-8945-1eb6b104f999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mStart Time:                2025-04-07 16:26:12.724244\n",
      "\u001b[0m\n",
      "\u001b[4mConsolidation Info\u001b[0m\n",
      "Merged file:               /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/1gram_files/6corpus/1gram-merged.jsonl.lz4\n",
      "Corpus file:               /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/1gram_files/6corpus/1gram-corpus.jsonl.lz4\n",
      "Temporary directory:       /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/1gram_files/temp_chunks\n",
      "Ngram size:                1\n",
      "Number of workers:         48\n",
      "Compress output files:     True\n",
      "Overwrite existing files:  True\n",
      "\n",
      "Created and Sorted: 5 chunks\n",
      "Merged: 5 chunks\n",
      "\n",
      "\u001b[31m\n",
      "End Time:                  2025-04-07 16:27:05.051701\u001b[0m\n",
      "\u001b[31mTotal runtime:             0:00:52.327457\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "consolidate_duplicate_ngrams(\n",
    "    ngram_size=1,\n",
    "    proj_dir='/vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction',\n",
    "    lines_per_chunk=500000,\n",
    "    compress=True,\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc294ed6-399f-4fbd-91e4-8e55e86950d0",
   "metadata": {},
   "source": [
    "### Index unigrams and create vocabulary file\n",
    "Most use cases will require an indexed list of \"valid\" (i.e., reasonably common) vocabulary words. This indexing script serves the dual functions of (1) mapping each unigram to an index number (saved in `/6corpus/1gram-corpus-indexed.jsonl`) and (2) culling this file into a vocabulary list consisting of the _n_ most frequent unigrams (saved in `6corpus/1gram-corpus-vocab_list_match.txt`). The vocabulary file provides a critical means of filtering excessively rare words out of the corpus. Unlike files upstream in the workflow, the vocabulary files are not large and don't need to be compressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e858de53-fe04-4a7d-bab5-9eed5cdf0011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mStart Time:                2025-04-07 16:27:33.930317\n",
      "\u001b[0m\n",
      "\u001b[4mIndexing Info\u001b[0m\n",
      "Corpus file:               /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/1gram_files/6corpus/1gram-corpus.jsonl.lz4\n",
      "Indexed file:              /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/1gram_files/6corpus/1gram-indexed.jsonl.lz4\n",
      "Ngram size:                1\n",
      "Number of workers:         48\n",
      "Compress output files:     True\n",
      "Overwrite existing files:  True\n",
      "\n",
      "\u001b[4mVocabulary Info\u001b[0m\n",
      "Vocab size (top N):        80000\n",
      "Match File:                /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/1gram_files/6corpus/1gram-corpus-vocab_list_match.txt\n",
      "Lookup File:               /vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/1gram_files/6corpus/1gram-corpus-vocab_list_lookup.jsonl\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e08be11db8f488badcfecfc6cbb1dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunking:   0%|          | 0/1125984 [00:00<?, ?lines/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daf94141676a4011a112b7fdec9c8ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sorting:   0%|          | 0/12 [00:00<?, ?chunks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b915aa545444353ab9b2207b3325553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Merging:   0%|          | 0/1125984 [00:00<?, ?lines/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c8ba2b22d14e1cab794a18a40b63c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Indexing:   0%|          | 0/1125984 [00:00<?, ?lines/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Indexed 1125984 lines.\n",
      "Final indexed file: 1gram-corpus-indexed.jsonl.lz4\n",
      "Created vocab_list_match and vocab_list_lookup files for top 80000 ngrams.\n",
      "\u001b[31m\n",
      "End Time:                  2025-04-07 16:28:41.158170\u001b[0m\n",
      "\u001b[31mTotal runtime:             0:01:07.227853\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "index_and_create_vocab_files(\n",
    "    ngram_size=1,\n",
    "    proj_dir='/vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction',\n",
    "    compress=True,\n",
    "    overwrite=True,\n",
    "    vocab_n=80000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5142c8d-50fa-4387-9383-f901492bace2",
   "metadata": {},
   "source": [
    "### Verify indexing [OPTIONAL]\n",
    "We can verify that the final indexed file looks right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2824a58d-213e-4c4a-8917-78909994239b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 40000: {'ngram': 'towelling', 'freq_tot': 36694, 'doc_tot': 25173, 'freq': {'1765': 1, '1812': 1, '1819': 1, '1823': 2, '1830': 2, '1833': 1, '1834': 4, '1836': 2, '1837': 4, '1840': 2, '1843': 1, '1846': 1, '1848': 4, '1850': 2, '1851': 3, '1852': 4, '1853': 3, '1854': 2, '1857': 6, '1858': 2, '1859': 8, '1860': 2, '1861': 22, '1862': 8, '1863': 3, '1864': 5, '1865': 33, '1866': 7, '1867': 6, '1868': 7, '1869': 6, '1870': 7, '1871': 12, '1872': 10, '1873': 2, '1874': 15, '1875': 10, '1876': 13, '1877': 7, '1878': 7, '1879': 13, '1880': 14, '1881': 8, '1882': 3, '1883': 20, '1884': 4, '1885': 10, '1886': 12, '1887': 7, '1888': 8, '1889': 7, '1890': 5, '1891': 15, '1892': 9, '1893': 4, '1894': 6, '1895': 6, '1896': 19, '1898': 20, '1899': 44, '1900': 23, '1901': 15, '1902': 19, '1903': 12, '1904': 9, '1905': 29, '1906': 19, '1907': 19, '1908': 24, '1909': 12, '1910': 24, '1911': 18, '1912': 18, '1913': 6, '1914': 14, '1915': 9, '1916': 8, '1917': 22, '1918': 12, '1919': 17, '1920': 19, '1921': 17, '1922': 22, '1923': 19, '1924': 11, '1925': 16, '1926': 20, '1927': 19, '1928': 15, '1929': 29, '1930': 18, '1931': 24, '1932': 15, '1933': 10, '1934': 9, '1935': 10, '1936': 5, '1937': 26, '1938': 18, '1939': 18, '1940': 8, '1941': 10, '1942': 11, '1943': 12, '1944': 7, '1945': 9, '1946': 6, '1947': 18, '1948': 33, '1949': 15, '1950': 26, '1951': 24, '1952': 31, '1953': 20, '1954': 16, '1955': 17, '1956': 15, '1957': 13, '1958': 22, '1959': 18, '1960': 24, '1961': 27, '1962': 11, '1963': 20, '1964': 31, '1965': 51, '1966': 22, '1967': 36, '1968': 65, '1969': 44, '1970': 73, '1971': 62, '1972': 55, '1973': 39, '1974': 48, '1975': 43, '1976': 63, '1977': 61, '1978': 72, '1979': 75, '1980': 88, '1981': 117, '1982': 175, '1983': 244, '1984': 238, '1985': 257, '1986': 259, '1987': 287, '1988': 289, '1989': 340, '1990': 306, '1991': 351, '1992': 310, '1993': 303, '1994': 315, '1995': 315, '1996': 364, '1997': 343, '1998': 343, '1999': 353, '2000': 410, '2001': 404, '2002': 442, '2003': 497, '2004': 444, '2005': 508, '2006': 605, '2007': 872, '2008': 1349, '2009': 1422, '2010': 2061, '2011': 2817, '2012': 3269, '2013': 2635, '2014': 2991, '2015': 2016, '2016': 1763, '2017': 1676, '2018': 1346, '2019': 1168, '1844': 1, '1849': 3, '1845': 1, '1897': 3}, 'doc': {'1765': 1, '1812': 1, '1819': 1, '1823': 2, '1830': 2, '1833': 1, '1834': 4, '1836': 2, '1837': 4, '1840': 2, '1843': 1, '1846': 1, '1848': 4, '1850': 2, '1851': 2, '1852': 4, '1853': 2, '1854': 2, '1857': 6, '1858': 2, '1859': 6, '1860': 2, '1861': 12, '1862': 8, '1863': 3, '1864': 3, '1865': 23, '1866': 5, '1867': 3, '1868': 6, '1869': 5, '1870': 6, '1871': 11, '1872': 8, '1873': 1, '1874': 10, '1875': 4, '1876': 7, '1877': 5, '1878': 5, '1879': 8, '1880': 9, '1881': 8, '1882': 3, '1883': 15, '1884': 4, '1885': 7, '1886': 12, '1887': 7, '1888': 5, '1889': 5, '1890': 4, '1891': 9, '1892': 5, '1893': 4, '1894': 5, '1895': 6, '1896': 12, '1898': 12, '1899': 36, '1900': 17, '1901': 13, '1902': 16, '1903': 9, '1904': 8, '1905': 27, '1906': 16, '1907': 13, '1908': 19, '1909': 12, '1910': 20, '1911': 16, '1912': 14, '1913': 6, '1914': 12, '1915': 5, '1916': 8, '1917': 22, '1918': 9, '1919': 14, '1920': 12, '1921': 12, '1922': 16, '1923': 16, '1924': 9, '1925': 12, '1926': 16, '1927': 16, '1928': 10, '1929': 23, '1930': 15, '1931': 23, '1932': 15, '1933': 10, '1934': 8, '1935': 9, '1936': 5, '1937': 16, '1938': 16, '1939': 15, '1940': 7, '1941': 10, '1942': 8, '1943': 11, '1944': 7, '1945': 9, '1946': 6, '1947': 15, '1948': 23, '1949': 13, '1950': 25, '1951': 20, '1952': 25, '1953': 19, '1954': 13, '1955': 12, '1956': 15, '1957': 12, '1958': 20, '1959': 18, '1960': 17, '1961': 18, '1962': 10, '1963': 18, '1964': 25, '1965': 30, '1966': 18, '1967': 33, '1968': 46, '1969': 34, '1970': 52, '1971': 37, '1972': 42, '1973': 32, '1974': 35, '1975': 30, '1976': 53, '1977': 54, '1978': 59, '1979': 57, '1980': 66, '1981': 78, '1982': 122, '1983': 168, '1984': 149, '1985': 169, '1986': 166, '1987': 207, '1988': 201, '1989': 220, '1990': 206, '1991': 221, '1992': 214, '1993': 202, '1994': 201, '1995': 222, '1996': 237, '1997': 220, '1998': 246, '1999': 242, '2000': 272, '2001': 293, '2002': 287, '2003': 336, '2004': 333, '2005': 376, '2006': 437, '2007': 591, '2008': 891, '2009': 973, '2010': 1430, '2011': 1830, '2012': 2134, '2013': 1782, '2014': 2026, '2015': 1322, '2016': 1170, '2017': 1151, '2018': 987, '2019': 896, '1844': 1, '1849': 3, '1845': 1, '1897': 1}, 'idx': 40000}\n"
     ]
    }
   ],
   "source": [
    "print_jsonl_lines(\n",
    "    file_path=(\n",
    "        '/vast/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/'\n",
    "        '1gram_files/6corpus/1gram-corpus-indexed.jsonl.lz4'\n",
    "    ),\n",
    "    start_line=40000,\n",
    "    end_line=40000,\n",
    "    parse_json=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694b77d7-8250-4edc-998d-da6ff9580fab",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "If you've gotten this far, you're ready to start pre-processing multigrams using the `workflow_multigrams.ipynb` notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hist_w2v_env",
   "language": "python",
   "name": "hist_w2v_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
