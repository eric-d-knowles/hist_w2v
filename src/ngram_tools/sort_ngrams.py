"""
sort_ngrams.py
--------------
MapReduce-style ngram bucketing and merging for large corpus files.

This module provides functions to bucketize ngram files in parallel using a list of bucket prefixes
generated by optimize_buckets, and to concatenate bucketed files into a single sorted output.

Main functions:
    - bucketize_all_files: Parallel bucketization of input files using provided bucket list.
    - bucketize_input_file: Assigns ngrams to buckets by prefix match.
    - concatenate_merged_buckets: Merges all bucket files into a single output file.

Usage:
    1. Generate bucket_list using optimize_buckets.
    2. Call bucketize_all_files with input paths, output directory, and bucket_list.
    3. Merge bucketed files with concatenate_merged_buckets.
"""

import os
import heapq
import math
import tempfile
import orjson
from concurrent.futures import ProcessPoolExecutor, as_completed
from datetime import datetime
from pathlib import Path
from glob import glob
from tqdm import tqdm
from ngram_tools.helpers.file_handler import FileHandler


def set_info(proj_dir, ngram_size, compress):
    # Set directory for filtered files
    filtered_dir = os.path.join(proj_dir, f'{ngram_size}gram_files/5filter')
    os.makedirs(filtered_dir, exist_ok=True)

    # Set directory for file-specific buckets
    buckets_dir = os.path.join(proj_dir, f'{ngram_size}gram_files/bucketed')
    os.makedirs(buckets_dir, exist_ok=True)
    
    # Set directory for omnibus buckets
    omnibus_dir = os.path.join(proj_dir, f'{ngram_size}gram_files/omnibus')
    os.makedirs(omnibus_dir, exist_ok=True)

    # Set path for concatenated output
    concat_dir = os.path.join(proj_dir, f'{ngram_size}gram_files/6corpus')
    os.makedirs(concat_dir, exist_ok=True)
    concat_path = os.path.join(concat_dir, f'{ngram_size}gram-merged.jsonl') + ('.lz4' if compress else '')

    return filtered_dir, buckets_dir, omnibus_dir, concat_path


def print_info(
    start_time,
    ngram_size,
    bucket_list,
    filtered_dir,
    buckets_dir,
    omnibus_dir,
    concat_path,
    workers_bucketize,
    workers_merge,
    workers_concat,
    compress,
    delete_filtered,
    delete_buckets,
    delete_omnibus
):
    print(f'\033[31mStart Time:                 {start_time}\n\033[0m')
    print('\033[4mMerge Info\033[0m')
    print(f'Ngram size:                 {ngram_size}')
    if isinstance(bucket_list, str):
        print(f'Bucket list:                {bucket_list} (filepath)')
    else:
        print(f'Bucket list:                [object passed]')
    print(f'Filtered files:             {filtered_dir}')
    print(f'File-specific buckets:      {buckets_dir}')
    print(f'Omnibus buckets:            {omnibus_dir}')
    print(f'Concatenated output:        {concat_path}')
    print(f'Workers for bucketing:      {workers_bucketize if workers_bucketize > 0 else "0 (skipping)"}')
    print(f'Workers for merging:        {workers_merge if workers_merge > 0 else "0 (skipping)"}')
    print(f'Workers for concatenation:  {workers_concat if workers_concat > 0 else "0 (skipping)"}')
    print(f'Compress output files:      {compress}')
    print(f'Delete filtered files:      {delete_filtered}')
    print(f'Delete bucket files:        {delete_buckets}')
    print(f'Delete omnibus files:       {delete_omnibus}\n')


def sort_and_combine(
    proj_dir,
    ngram_size,
    bucket_list,
    compress=True,
    workers_bucketize=os.cpu_count(),
    workers_merge=os.cpu_count(),
    workers_concat=os.cpu_count(),
    delete_filtered=False,
    delete_buckets=False,
    delete_omnibus=False
):
    """
    Run the full ngram bucketing and concatenation pipeline.

    Args:
        proj_dir (str): Project directory containing ngram files.
        ngram_size (int): Size of the ngrams (e.g., 5 for 5-grams).
        bucket_list (list): List of bucket prefixes to use (from optimize_buckets).
        compress (bool): Whether to compress output files.
        delete_filtered (bool): Delete input files after bucketization.
        delete_buckets (bool): Delete bucket files after omnibus merge.
        delete_omnibus (bool): Delete omnibus files after final concatenation.
        workers_bucketize (int): Number of parallel workers for bucketization.
        workers_merge (int): Number of parallel workers for merging buckets.
        workers_concat (int): Number of parallel workers for concatenation.

    Returns:
        int: Number of bucket files concatenated.

    Note:
        bucket_list should be a list of bucket prefixes, e.g. ['a', 'b', ...], not a dictionary.
    """
    start_time = datetime.now()

    filtered_dir, buckets_dir, omnibus_dir, concat_path = set_info(
        proj_dir, ngram_size, compress
    )

    # Print initial information
    print_info(
        start_time,
        ngram_size,
        bucket_list,
        filtered_dir,
        buckets_dir,
        omnibus_dir,
        concat_path,
        workers_bucketize,
        workers_merge,
        workers_concat,
        compress,
        delete_filtered,
        delete_buckets,
        delete_omnibus
    )

    # Step 1: Bucketize files unless skipping
    
    # If bucket_list is a string, treat it as a path to a JSON file and load it
    if isinstance(bucket_list, str):
        import json
        with open(bucket_list, 'r', encoding='utf-8') as f:
            bucket_list = json.load(f)

    # Get all filtered paths
    filtered_paths = sorted(
        glob(os.path.join(filtered_dir, '*.jsonl')) +
        glob(os.path.join(filtered_dir, '*.jsonl.lz4'))
    )

    if workers_bucketize > 0:
        bucketize_all_files(
            filtered_paths,
            buckets_dir,
            compress=compress,
            workers=workers_bucketize,
            bucket_list=bucket_list,
            delete_filtered= delete_filtered
        )
    else:
        print("Skipping bucketing step.")

    # Step 2: Merge buckets unless skipping
    if workers_merge > 0:
        merge_all_buckets(
            bucket_list,
            buckets_dir,
            omnibus_dir,
            compress,
            delete_buckets,
            workers_merge
        )
    else:
        print("Skipping merging step.")

    # Step 3: Concatenate merged buckets
    if workers_concat > 0:
        concatenate_merged_buckets(
            omnibus_dir,
            concat_path,
            compress=compress,
            delete_omnibus=delete_omnibus,
            workers_concat=workers_concat
        )
    else:
        print("Skipping concatenation step.")
    
    end_time = datetime.now()
    print(f'\033[31m\nEnd Time:                  {end_time}\033[0m')

    total_runtime = end_time - start_time
    print(f'\033[31mTotal runtime:             {total_runtime}\n\033[0m')

    return    


def merge_all_buckets(
    bucket_list,
    output_dir,
    omnibus_dir,
    compress,
    delete_buckets,
    workers
):
    """
    Merge all bucket files in parallel into omnibus files.

    Args:
        bucket_list (list): List of bucket prefixes to merge.
        output_dir (str): Directory containing bucketed files.
        omnibus_dir (str): Directory to store merged omnibus files.
        compress (bool): Whether to compress output files.
        delete_buckets (bool): Delete bucket files after merging.
        workers (int): Number of parallel workers.

    Returns:
        int: Number of buckets merged.
    """
    from concurrent.futures import ProcessPoolExecutor, as_completed

    futures = []
    with ProcessPoolExecutor(max_workers=workers) as executor:
        for bucket in bucket_list:
            futures.append(
                executor.submit(
                    merge_bucket,
                    bucket,
                    output_dir,
                    omnibus_dir,
                    compress,
                    delete_buckets
                )
            )
        with tqdm(total=len(futures), desc="Merging buckets", unit="bucket") as pbar:
            for future in as_completed(futures):
                try:
                    future.result()
                except Exception as e:
                    print(f"Exception in merging bucket: {e}")
                pbar.update(1)
    print(f"Heap-merged {len(bucket_list)} buckets into {omnibus_dir}")
    return len(bucket_list)


def merge_bucket(bucket, output_dir, omnibus_dir, compress, delete_buckets=False):
    """
    Heap-merge all file-specific buckets into one sorted omnibus file per bucket.
    Args:
        bucket: Bucket prefix (str or tuple).
        output_dir: Directory containing bucketed files.
        omnibus_dir: Directory to store merged omnibus files.
        compress: Whether to compress output files.
        delete_buckets: Delete bucket files after merging.
    Returns:
        bucket_prefix (str) if merged, else None.
    """
    bucket_prefix = bucket[0] if isinstance(bucket, (tuple, list)) else bucket
    bucket_dir = os.path.join(output_dir, str(bucket_prefix))
    pattern = "*.jsonl" + (".lz4" if compress else "")
    bucket_files = sorted(glob(os.path.join(bucket_dir, pattern)))
    if not bucket_files:
        return None
    out_name = f"{bucket_prefix}_omnibus.jsonl" + (".lz4" if compress else "")
    out_path = os.path.join(omnibus_dir, out_name)
    if compress:
        import lz4.frame
        def open_fn(path):
            return lz4.frame.open(path, 'rb')
    else:
        def open_fn(path):
            return open(path, 'r', encoding='utf-8')

    def gen_entries(path):
        with open_fn(path) as f:
            for line in f:
                try:
                    entry = orjson.loads(line if isinstance(line, bytes) else line.encode('utf-8'))
                except Exception:
                    continue  # skip malformed
                key = ' '.join([
                    entry.get('ngram', {}).get(f'token{i + 1}', '')
                    for i in range(5)
                ]).lower()
                yield (key, line)

    def entry_iterators():
        for f in bucket_files:
            yield gen_entries(f)

    def heap_entry_merge(*iterators):
        # Merge by key, output line
        for _, line in heapq.merge(*iterators, key=lambda x: x[0]):
            yield line

    with (
        lz4.frame.open(out_path, 'wb') if compress else open(out_path, 'w', encoding='utf-8')
    ) as fout:
        for line in heap_entry_merge(*entry_iterators()):
            fout.write(line)

    if delete_buckets:
        for f in bucket_files:
            try:
                os.remove(f)
            except Exception as e:
                print(f"Warning: Could not delete bucket file {f}: {e}")
    return bucket_prefix


def _bucketize_one(input_path, output_dir, compress, bucket_list=None, delete_filtered=False):
    """
    Helper for parallel bucketization of a single file.

    Args:
        input_path (str): Path to the input file.
        output_dir (str): Directory to store bucketized files.
        compress (bool): Whether to compress output files.
        bucket_list (list): List of bucket prefixes to use.
    {delete_filtered}') (bool): Delete input file after bucketization.

    Returns:
        tuple: (input_path, bucket_paths)
    """
    bucket_paths = bucketize_input_file(
        input_path, output_dir, compress, bucket_list=bucket_list
    )
    if delete_filtered:
        try:
            os.remove(input_path)
        except Exception as e:
            print(f"Warning: Could not delete input file {input_path}: {e}")
    return input_path, bucket_paths


def bucketize_all_files(
    input_paths,
    output_dir,
    compress,
    workers=os.cpu_count(),
    bucket_list=None,
    delete_filtered=False
):
    """
    Bucketize all input files in parallel, storing bucketized files in output_dir.

    Args:
        input_paths (list[str]): List of input file paths to bucketize.
        output_dir (str): Directory to store bucketized files.
        compress (bool): Whether to compress output files.
        workers (int): Number of parallel workers.
        bucket_list (list): List of bucket prefixes to use (from optimize_buckets).
    {delete_filtered}') (bool): Delete input files after bucketization.

    Returns:
        dict: Mapping from input file to its bucket output paths (dict per file).
    """
    os.makedirs(output_dir, exist_ok=True)
    from concurrent.futures import ProcessPoolExecutor, as_completed
    results = {}
    with ProcessPoolExecutor(max_workers=workers) as executor:
        futures = [
            executor.submit(
                _bucketize_one,
                path,
                output_dir,
                compress,
                bucket_list,
                delete_filtered
            ) for path in input_paths
        ]
        for future in tqdm(
            as_completed(futures),
            total=len(futures),
            desc="Bucketing",
            unit="files"
        ):
            input_path, bucket_paths = future.result()
            results[input_path] = bucket_paths
    return results


def concatenate_merged_buckets(
    omnibus_dir,
    concat_path,
    compress=True,
    delete_omnibus=False,
    workers_concat=1
):
    os.makedirs(os.path.dirname(concat_path), exist_ok=True)
    pattern = '*.jsonl.lz4' if compress else '*.jsonl'
    bucket_files = sorted(glob(os.path.join(omnibus_dir, pattern)))
    
    print('\n')
    if not bucket_files:
        print(f"[WARNING] No omnibus files found in {omnibus_dir}. Skipping concatenation.")
        return 0
    if compress:
        import lz4.frame
    if workers_concat <= 1 or len(bucket_files) <= 1:
        # Single-threaded fallback
        if compress:
            with lz4.frame.open(concat_path, 'wb') as fout:
                for bucket_file in bucket_files:
                    with lz4.frame.open(bucket_file, 'rb') as fin:
                        while True:
                            chunk = fin.read(1024 * 1024)
                            if not chunk:
                                break
                            fout.write(chunk)
                    if delete_omnibus:
                        try:
                            os.remove(bucket_file)
                        except Exception as e:
                            print(f"Warning: Could not delete omnibus file {bucket_file}: {e}")
        else:
            with open(concat_path, 'wb') as fout:
                for bucket_file in bucket_files:
                    with open(bucket_file, 'rb') as fin:
                        while True:
                            chunk = fin.read(1024 * 1024)
                            if not chunk:
                                break
                            fout.write(chunk)
                    if delete_omnibus:
                        try:
                            os.remove(bucket_file)
                        except Exception as e:
                            print(f"Warning: Could not delete omnibus file {bucket_file}: {e}")
        print(f"Concatenated {len(bucket_files)} buckets into {concat_path}")
        return len(bucket_files)

    # Parallel merge: divide files into groups, merge each group, then merge results
    group_size = math.ceil(len(bucket_files) / workers_concat)
    groups = [bucket_files[i:i+group_size] for i in range(0, len(bucket_files), group_size)]
    intermediate_files = []

    with tempfile.TemporaryDirectory() as tmpdir:
        with ProcessPoolExecutor(max_workers=workers_concat) as executor:
            futures = []
            for i, group in enumerate(groups):
                out_path = os.path.join(tmpdir, f"intermediate_{i}.jsonl" + (".lz4" if compress else ""))
                intermediate_files.append(out_path)
                futures.append(executor.submit(merge_group, group, out_path, compress))
            # Progress bar for creation of intermediate files
            with tqdm(total=len(futures), desc="Creating intermediate files", unit="file") as pbar:
                for future in as_completed(futures):
                    future.result()
                    pbar.update(1)

        # Progress bar for concatenating intermediate files
        import sys
        if compress:
            with lz4.frame.open(concat_path, 'wb') as fout:
                with tqdm(total=len(intermediate_files), desc="Concatenating intermediate files", unit="file", file=sys.stdout) as pbar:
                    for int_file in intermediate_files:
                        with lz4.frame.open(int_file, 'rb') as fin:
                            while True:
                                chunk = fin.read(1024 * 1024)
                                if not chunk:
                                    break
                                fout.write(chunk)
                        pbar.update(1)
                        sys.stdout.flush()
        else:
            with open(concat_path, 'wb') as fout:
                with tqdm(total=len(intermediate_files), desc="Concatenating intermediate files", unit="file", file=sys.stdout) as pbar:
                    for int_file in intermediate_files:
                        with open(int_file, 'rb') as fin:
                            while True:
                                chunk = fin.read(1024 * 1024)
                                if not chunk:
                                    break
                                fout.write(chunk)
                        pbar.update(1)
                        sys.stdout.flush()
    print(f"Concatenated {len(bucket_files)} buckets into {concat_path} using {workers_concat} workers")
    return len(bucket_files)

# Move merge_group to top level
def merge_group(files, out_path, compress):
    if compress:
        import lz4.frame
        with lz4.frame.open(out_path, 'wb') as fout:
            for f in files:
                with lz4.frame.open(f, 'rb') as fin:
                    while True:
                        chunk = fin.read(1024 * 1024)
                        if not chunk:
                            break
                        fout.write(chunk)
    else:
        with open(out_path, 'wb') as fout:
            for f in files:
                with open(f, 'rb') as fin:
                    while True:
                        chunk = fin.read(1024 * 1024)
                        if not chunk:
                            break
                        fout.write(chunk)
    return out_path


def bucketize_input_file(input_path, output_dir, compress, bucket_list=None):
    """
    Reads an input file and writes its records into bucketed files in output_dir.

    Each bucketed file contains records assigned to that bucket (using provided bucket_list).

    Args:
        input_path (str): Path to the input file.
        output_dir (str): Directory to write bucketed files.
        compress (bool): Whether to compress output files.
        bucket_list (list): List of bucket prefixes to use (from optimize_buckets).

    Returns:
        dict: Mapping from bucket name to output file path.
    """
    

    bucket_paths = {}
    file_handler = FileHandler(input_path)
    stem = Path(input_path).stem
    if stem.endswith('.jsonl'):
        stem = stem[:-6]

    # Prepare a dictionary to collect entries for each bucket
    bucket_entries = {bucket[0] if isinstance(bucket, (tuple, list)) else bucket: [] for bucket in bucket_list}

    # Read input file and assign each entry to the appropriate bucket
    with file_handler.open() as infile:
        for line in infile:
            entry = file_handler.deserialize(line)
            ngram_dict = entry.get('ngram', {})
            if not isinstance(ngram_dict, dict):
                continue  # Skip malformed entries

            tokens = [ngram_dict.get(f'token{i + 1}', '') for i in range(5)]
            first_chars = [token[0].lower() for token in tokens if token]

            for bucket in bucket_list:
                bucket_prefix = bucket[0] if isinstance(bucket, (tuple, list)) else bucket
                bucket_chars = list(bucket_prefix.lower())
                if first_chars[:len(bucket_chars)] == bucket_chars:
                    bucket_entries[bucket_prefix].append(entry)
                    break

    # Sort and write entries to bucket files
    for bucket in bucket_list:
        bucket_prefix = bucket[0] if isinstance(bucket, (tuple, list)) else bucket
        entries = bucket_entries[bucket_prefix]
        if not entries:
            continue  # Skip writing empty files
        entries.sort(
            key=lambda entry: ' '.join([
                entry.get('ngram', {}).get(f'token{i + 1}', '')
                for i in range(5)
            ]).lower()
        )

        bucket_dir = os.path.join(output_dir, str(bucket_prefix))
        os.makedirs(bucket_dir, exist_ok=True)
        out_name = f"{stem}.jsonl" + (".lz4" if compress else "")
        out_path = os.path.join(bucket_dir, out_name)
        bucket_paths[bucket_prefix] = out_path

        handler = FileHandler(out_path, is_output=True, compress=compress).open()
        for entry in entries:
            handler.write(file_handler.serialize(entry))
        handler.close()

    return bucket_paths
